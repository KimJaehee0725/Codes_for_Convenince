{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq with attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ORO_qUez3jLOblcaQkre-bS5VmEMox8K",
      "authorship_tag": "ABX9TyMsc6IMdllA0L6bbScIxiU4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FErJNXqLrAOr"
      },
      "source": [
        "# 0. Preprocessing\n",
        "- 파일 로드하기(File Loading) : 다양한 포맷의 코퍼스를 로드합니다.\n",
        "- 토큰화(Tokenization) : 문장을 단어 단위로 분리해줍니다.\n",
        "- 단어 집합(Vocab) : 단어 집합을 만듭니다.\n",
        "- 정수 인코딩(Integer encoding) : 전체 코퍼스의 단어들을 각각의 고유한 정수로 맵핑합니다.\n",
        "- 단어 벡터(Word Vector) : 단어 집합의 단어들에 고유한 임베딩 벡터를 만들어줍니다. 랜덤값으로 초기화한 값일 수도 있고, 사전 훈련된 임베딩 벡터들을 로드할 수도 있습니다.\n",
        "- 배치화(Batching) : 훈련 샘플들의 배치를 만들어줍니다. 이 과정에서 패딩 작업(Padding)도 이루어집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFoL9iNkCS3T",
        "outputId": "1ed4a6a1-1dfe-4c3e-c688-48b90c59847f"
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "# colab으로 mecab 실행 시 (설치 코드)\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n",
            "/content/Mecab-ko-for-Google-Colab/Mecab-ko-for-Google-Colab/Mecab-ko-for-Google-Colab\n",
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-06-18 12:12:34--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 18.205.93.2, 18.205.93.1, 18.205.93.0, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|18.205.93.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=qc53y9LS1IjWhWJrD%2BJ8auUxgr8%3D&Expires=1624019915&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-06-18 12:12:34--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=qc53y9LS1IjWhWJrD%2BJ8auUxgr8%3D&Expires=1624019915&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.140.196\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.140.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz.3’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-06-18 12:12:34 (31.7 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz.3’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-06-18 12:12:50--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::6b17:d1f5, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=datrQZfxp9%2Fy95o%2BzoqynSxvR0k%3D&Expires=1624020008&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-06-18 12:12:51--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=datrQZfxp9%2Fy95o%2BzoqynSxvR0k%3D&Expires=1624020008&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.163.249\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.163.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz.3’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M   110MB/s    in 0.4s    \n",
            "\n",
            "2021-06-18 12:12:51 (110 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz.3’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFed7qsarAFO"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import copy\n",
        "import time\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from konlpy.tag import Mecab\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import Parameter\n",
        "from torchtext.vocab import Vocab\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQWFl1IMBHRi"
      },
      "source": [
        "dim_embed = 128\n",
        "dim_model = 64\n",
        "batch_size = 16\n",
        "num_layers = 2\n",
        "is_bidirection = True\n",
        "truncated = 5\n",
        "dropout_ratio = 0.3\n",
        "base_path = '/content/drive/MyDrive/04_프로젝트/seq2seq with attention/'\n",
        "device = torch.device('cuda' if True and torch.cuda.is_available() else 'cpu')\n",
        "epochs = 30\n",
        "clip = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY_VQnevroaC"
      },
      "source": [
        "# path = '/content/drive/MyDrive/04_프로젝트/seq2seq with attention/dataset/'\n",
        "# os.chdir(path)\n",
        "# save_name = 'unzipped/'\n",
        "# file_list = os.listdir(path)\n",
        "# for file in file_list:\n",
        "#   tar = tarfile.open(file)\n",
        "#   tar.extractall('./' + save_name + file.split(\".\")[0])\n",
        "#   tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyOHYJSsbsIu"
      },
      "source": [
        "en_tokenizer = get_tokenizer('spacy')\n",
        "ko_tokenizer = Mecab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ_sojwh2QEE"
      },
      "source": [
        "path_base = \"/content/drive/MyDrive/04_프로젝트/seq2seq with attention/dataset/unzipped/korean-english-park/korean-english-park.\"\n",
        "train_path = ('train.en', 'train.ko')\n",
        "dev_path = ('dev.en', 'dev.ko')\n",
        "test_path = ('test.en', 'test.ko')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8A2Kuzg2Z2G"
      },
      "source": [
        "# 출처 : https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
        "ko_tokenizer = Mecab()\n",
        "en_tokenizer = get_tokenizer('spacy')\n",
        "def build_vocab(train_path, tokenizer):\n",
        "  counter = Counter()\n",
        "  with open(train_path, encoding = 'UTF-8', newline = '\\n') as f:\n",
        "    for string_ in f:\n",
        "      if 'ko' in train_path[-10:]:\n",
        "        counter.update(tokenizer.morphs(string_))\n",
        "      else:\n",
        "        counter.update(tokenizer(string_))\n",
        "    return Vocab(counter, min_freq = 3, specials = ('<unk>', '<BOS>', '<EOS>', \"<PAD>\"))\n",
        "\n",
        "ko_vocab = build_vocab(path_base + train_path[1], ko_tokenizer)\n",
        "en_vocab = build_vocab(path_base + train_path[0], en_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqgxl4K23x0w"
      },
      "source": [
        "def data_preprocess(file_paths):\n",
        "  raw_ko_iter = iter(io.open(file_paths[1], encoding = 'UTF-8', newline = '\\n'))\n",
        "  raw_en_iter = iter(io.open(file_paths[0], encoding = 'UTF-8', newline = '\\n'))\n",
        "  data = []\n",
        "  for raw_ko, raw_en in zip(raw_ko_iter, raw_en_iter):\n",
        "    ko_tensor = torch.tensor([ko_vocab[token] for token in ko_tokenizer.morphs(raw_ko)], dtype = torch.long)\n",
        "    en_tensor = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype = torch.long)\n",
        "    data.append((ko_tensor, en_tensor))\n",
        "\n",
        "  return data\n",
        "train_data= data_preprocess([path_base + path for path in train_path])\n",
        "dev_data = data_preprocess([path_base + path for path in dev_path])\n",
        "test_data = data_preprocess([path_base + path for path in test_path])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W90ltLrB6NF8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "PAD_IDX = ko_vocab['<PAD>']\n",
        "BOS_IDX = ko_vocab['<BOS>']\n",
        "EOS_IDX = ko_vocab['<EOS>']\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  ko_batch, en_batch = [], []\n",
        "  for (ko_item, en_item) in data_batch:\n",
        "    ko_batch.append(torch.cat([torch.tensor([BOS_IDX]), ko_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  ko_batch = pad_sequence(ko_batch, padding_value=PAD_IDX, batch_first = True)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first = True)\n",
        "  return ko_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "dev_iter = DataLoader(dev_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oRy8Lpcq9d3"
      },
      "source": [
        "# 1. modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzQM_jJG3XiX"
      },
      "source": [
        "## 1-1. Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsnRRkl648Kn"
      },
      "source": [
        "class gru_cell(nn.Module):\n",
        "  def __init__(self, dim_model, dim_embed, batch_size):\n",
        "    super().__init__()\n",
        "    self.W_z = nn.Linear(dim_embed, dim_model)\n",
        "    self.W_r = nn.Linear(dim_embed, dim_model)\n",
        "    self.W_ = nn.Linear(dim_embed, dim_model)\n",
        "    self.U_z = nn.Linear(dim_model, dim_model)\n",
        "    self.U_r = nn.Linear(dim_model, dim_model)\n",
        "    self.U_ = nn.Linear(dim_model, dim_model)\n",
        "    self.h = torch.zeros(dim_model, device = device).repeat(batch_size, 1)\n",
        "  def forward(self, x, h_old = None): # x : 모든 배치의 한 시점의 단어 임베딩 벡터 (batch_size, dim_embed)\n",
        "    if h_old is None:\n",
        "      h_old = self.h\n",
        "    r = torch.sigmoid(self.W_r(x) + self.U_r(h_old))\n",
        "    z = torch.sigmoid(self.W_z(x) + self.U_z(h_old))\n",
        "    h_new = torch.tanh(self.W_(x) + self.U_(r*h_old))\n",
        "    h = (1 - z)*h_old + z*h_new\n",
        "    return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbzJ37zRwr6G"
      },
      "source": [
        "class gru(nn.Module):\n",
        "  def __init__(self, dim_model, dim_embed, batch_size, gru_cell, truncated):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(gru_cell(dim_model, dim_embed, batch_size))\n",
        "    self.truncated = truncated\n",
        "    # self.bptt_truncated = bptt_truncated # bptt를 몇 시점마다 짜를지\n",
        "  def forward(self, x, max_seq, return_states = True, backward = False):\n",
        "    h = None\n",
        "    states = []\n",
        "    input = x.permute(1, 0, 2) # (time_step, batch_size, dim_model) 로 바꿈.\n",
        "    for time_step in range(max_seq):\n",
        "      if backward: # 역방향 레이어면, 맨 뒤의 값부터 입력\n",
        "        time_step = max_seq - time_step - 1\n",
        "      h = self.layer[0].forward(input[time_step], h)\n",
        "      if ((max_seq - self.truncated) % self.truncated == 0): # truncated BPTT, truncated 마다 h의 backprop을 끊음으로써 구현\n",
        "        h.detach()\n",
        "      if backward: # 역방향 레이어면 출력 순서도 역방향이므로 다시 돌려서 저장\n",
        "        states.insert(0, h)\n",
        "      else: \n",
        "        states.append(h)\n",
        "          \n",
        "    states = torch.stack(states).permute(1, 0, 2)\n",
        "    if return_states:\n",
        "      return states\n",
        "    else: \n",
        "      return self.h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR5pIYwtjRG3"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, dim_model, dim_embed, batch_size, num_layers, gru, gru_cell, truncated, bidirection = False):\n",
        "    super().__init__()\n",
        "    self.direction = 2 if bidirection else 1\n",
        "    rest_layer_input = 2*dim_model if bidirection else dim_model\n",
        "    first_forward_floor = gru(dim_model, dim_embed, batch_size, gru_cell, truncated)\n",
        "    forward_layers = [gru(dim_model, rest_layer_input, batch_size, gru_cell, truncated) for layer in range(num_layers - 1)]\n",
        "    forward_layers.insert(0, first_forward_floor)\n",
        "    self.forward_layers = nn.ModuleList(forward_layers)\n",
        "    if self.direction == 2:\n",
        "      first_backward_floor = gru(dim_model, dim_embed, batch_size, gru_cell, truncated)\n",
        "      backward_layers = [gru(dim_model, rest_layer_input, batch_size, gru_cell, truncated) for layer in range(num_layers - 1)]\n",
        "      backward_layers.insert(0, first_forward_floor)\n",
        "      self.backward_layers = nn.ModuleList(backward_layers)\n",
        "    self.max_seq = max_seq\n",
        "    self.num_layers = num_layers\n",
        "  def forward(self, x, return_states = False): # x : 모든 배치의 모든 시점의 단어 임베딩 벡터 (batch_size, max_len, dim_embed)\n",
        "    input_list = x\n",
        "    max_seq = x.size()[1]\n",
        "    h_output = []\n",
        "    for floor in range(self.num_layers): # 양방향 모델로 만들 경우 이전 층의 양 방향 hidden state를 입력값으로 받기 위해 forward 메소드 내에 시점 반영\n",
        "      h_output_layer = []\n",
        "      h_forward = self.forward_layers[floor].forward(input_list, max_seq)\n",
        "      h_output_layer.append(h_forward)\n",
        "      if self.direction == 2:        \n",
        "        h_backward = self.backward_layers[floor].forward(input_list, max_seq)\n",
        "        h_output_layer.append(h_backward)\n",
        "      h_output.append(h_output_layer)\n",
        "      input_list = torch.cat(h_output_layer, dim = 2)\n",
        "    self.states = torch.cat(h_output_layer, dim = 2)\n",
        "    return self.states, self.states[:, -1, :] # 마지막 시점의  hidden state를 따로 뽑음"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulk_hiMV3cCd"
      },
      "source": [
        "## 1-2. Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2MAWxF54m9A"
      },
      "source": [
        "dim_embed = 512\n",
        "vocab_size = 1000\n",
        "dim_model = 128\n",
        "max_seq = 50\n",
        "batch_size = 16\n",
        "encoder_bidirectional = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240NPVYe5sP_"
      },
      "source": [
        "class attention(nn.Module):\n",
        "  def __init__(self, dim_model, batch_size):\n",
        "    super().__init__()\n",
        "    self.W = nn.Linear(2*dim_model, dim_model)\n",
        "    self.U = nn.Linear(2*dim_model, dim_model)\n",
        "    self.v = nn.Linear(dim_model, 1)\n",
        "    self.batch_size = batch_size\n",
        "  def get_attention(self, hidden_state, encoding_matrix): # hidden state : (batch_size, 2*dim_model), encoding_matrix : (bath_size, max_seq, 2*dim_model)\n",
        "    if len(hidden_state.size()) == 2 :\n",
        "      hidden_state = hidden_state.unsqueeze(1) # (batch_size, 1, 2*dim_model)\n",
        "    max_seq = encoding_matrix.size()[1]\n",
        "    attention_score = self.v(torch.tanh(self.W(hidden_state) + self.U(encoding_matrix))) # (batch_size, max_seq, 1)\n",
        "    attention_dist = torch.softmax(attention_score, dim = 1) # (batch_size, max_seq, 1)\n",
        "    attention_vect = (attention_dist.expand(self.batch_size, max_seq, 2*dim_model)*encoding_matrix).sum(dim = 1)\n",
        "    return attention_vect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gglmxTQH3bEW"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, dim_model, dim_embed, batch_size, encoder_bidirectional = True):\n",
        "    super().__init__()\n",
        "    self.direction = 2 if encoder_bidirectional else 1\n",
        "    dim_decoder = self.direction*dim_model\n",
        "    self.W_z = nn.Linear(dim_embed, dim_decoder)\n",
        "    self.W_r = nn.Linear(dim_embed, dim_decoder)\n",
        "    self.W_s = nn.Linear(dim_embed, dim_decoder)\n",
        "\n",
        "    self.U_z = nn.Linear(2*dim_model, dim_decoder)\n",
        "    self.U_r = nn.Linear(2*dim_model, dim_decoder)\n",
        "    self.U_s = nn.Linear(2*dim_model, dim_decoder)\n",
        "\n",
        "    self.V_z = nn.Linear(2*dim_model, dim_decoder)\n",
        "    self.V_r = nn.Linear(2*dim_model, dim_decoder)\n",
        "    self.V_s = nn.Linear(2*dim_model, dim_decoder)\n",
        "\n",
        "    self.attention = nn.ModuleList([attention(dim_model, batch_size)])\n",
        "  def forward(self, x, h_old, encoding_matrix): # x : 모든 배치의 이전 시점 출력 단어 임베딩 벡터 (batch_size, dim_embed) / h_old 이전 시점 hidden state (batch_size, 2*dim_model)\n",
        "    attention_vect = self.attention[0].get_attention(h_old, encoding_matrix) # (batch_size, 2*dim_embed)\n",
        "    r = torch.sigmoid(self.W_r((x)) + self.U_r(h_old) + self.V_r(attention_vect))\n",
        "    z = torch.sigmoid(self.W_z((x)) + self.U_z(h_old) + self.V_z(attention_vect))\n",
        "    h_new = torch.tanh(self.W_s((x)) + self.U_s(r*h_old) + self.V_s(attention_vect))\n",
        "    h = (1 - z)*h_old + z*h_new\n",
        "    return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7JCOTEcuIGy"
      },
      "source": [
        "## 3. Full Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9dRoPjmXH-W"
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, Encoder, Decoder, dim_model, dim_embed, batch_size, num_layers, gru, gru_cell, is_bidirection, truncated, dropout_ratio, encoder_vocab, decoder_vocab, device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.encoder_vocab = encoder_vocab\n",
        "    self.decoder_vocab = decoder_vocab\n",
        "    self.encoder_vocab_size = len(self.encoder_vocab)\n",
        "    self.decoder_vocab_size = len(self.decoder_vocab) \n",
        "\n",
        "    encoder_partial = Encoder(dim_model, dim_embed, batch_size, 3, gru, gru_cell, truncated, bidirection = is_bidirection)\n",
        "    encoder_embedding = nn.Embedding(self.encoder_vocab_size, dim_embed)\n",
        "    encoder_dropout = Dropout(dropout_ratio)\n",
        "    encoder = nn.Sequential(encoder_embedding, encoder_dropout, encoder_partial)\n",
        "\n",
        "    decoder = Decoder(dim_model, dim_embed, batch_size, encoder_bidirectional = is_bidirection)\n",
        "    linear_input_dim = 2*dim_model if is_bidirection else dim_model\n",
        "    decoder_linear = nn.Linear(linear_input_dim, self.decoder_vocab_size)\n",
        "    \n",
        "    self.layers = nn.ModuleDict({\"encoder\" : encoder, \"decoder\" : decoder, \"dense\" : decoder_linear})\n",
        "\n",
        "\n",
        "    self.__sos_token_index = torch.tensor(self.decoder_vocab.stoi[\"<BOS>\"], device = self.device)\n",
        "    self.__eos_token_index = torch.tensor(self.decoder_vocab.stoi[\"<EOS>\"], device = self.device)\n",
        "    self.__pad_token_index = torch.tensor(self.decoder_vocab.stoi['<PAD>'], device = self.device)\n",
        "    self.batch_size = batch_size\n",
        "    self.decoder_embedding = nn.Sequential(nn.Embedding(num_embeddings = self.decoder_vocab_size, embedding_dim = dim_embed, padding_idx = self.__pad_token_index))   \n",
        "    \n",
        "    \n",
        "\n",
        "  def forward(self, input_tokens, gold_tokens, teach_forcing_ratio = 0.7): # input_tokens : 번역할 문장 (batch_size, max_seq) gold_token : 실제 정답 토큰 <sos> 부착 상태 (batch_size, max_seq_decoder)\n",
        "    gold_embed = self.decoder_embedding(gold_tokens) # (batch_size, max_seq, dim_embed)\n",
        "    encoder_max_seq = input_tokens.size()[-1]\n",
        "    encoding_matrix, hidden_state = self.layers.encoder.forward(input_tokens)\n",
        "\n",
        "    output_embed = self.decoder_embedding(self.__sos_token_index).squeeze(0).expand(self.batch_size, dim_embed) # (batch_size, dim_embed)\n",
        "    max_seq_decoder = gold_tokens.size()[-1]\n",
        "    teach_forcing_prob = torch.rand(max_seq_decoder)\n",
        "\n",
        "    sentence_made = torch.zeros((batch_size, max_seq_decoder, self.decoder_vocab_size))\n",
        "\n",
        "    for seq in range(max_seq_decoder):\n",
        "      hidden_state = self.layers.decoder.forward(output_embed, hidden_state, encoding_matrix)  #(batch_size, 2*dim_model)\n",
        "      \n",
        "      output_token_vect = self.layers.dense(hidden_state)\n",
        "      sentence_made[:, seq, :] = output_token_vect # 손실 함수를 위해 출력할 값 저장. 이 값은 softmax 통과하기 전 (batch_size, decoder_vocab_size)\n",
        "      output_token = output_token_vect.argmax(dim = 1) # 어차피 소프트맥스 하나 안하나 이 시점의 가장 큰 값이 output token\n",
        "      output_embed = self.decoder_embedding(output_token)\n",
        "      output_embed = gold_embed[:, seq, :] if (teach_forcing_prob[seq] < teach_forcing_ratio) else output_embed # teacher forcing을 사용할지 정해짐\n",
        "\n",
        "    return sentence_made \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJvFpyf1sKSI"
      },
      "source": [
        "model = Model(Encoder, Decoder, dim_model, dim_embed, batch_size, num_layers, gru, gru_cell, is_bidirection, truncated, dropout_ratio, encoder_vocab = en_vocab, decoder_vocab = ko_vocab, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rx0vU2Qtmhi",
        "outputId": "a326785a-6716-4d44-c89a-3902c41b060a"
      },
      "source": [
        "# count_parameters 함수 출처 : https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 10,831,532 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcyjAy2_jqrh"
      },
      "source": [
        "# 2. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoyvQUxFe1gj"
      },
      "source": [
        "class early_stopping:\n",
        "  def __init__(self, patience = 10, save_path = \"./save_model\"):\n",
        "    self.patience = patience\n",
        "    self.save_path = save_path\n",
        "    self.count = 0\n",
        "    self.best_score = np.Inf\n",
        "    self.stop = False\n",
        "    self.best_model = None\n",
        "\n",
        "  def __call__(self, val_loss, model):\n",
        "    if self.best_score == None:\n",
        "      self.best_score = val_loss\n",
        "      self.save_model(model)\n",
        "    elif val_loss < self.best_score:\n",
        "      self.best_score = val_loss\n",
        "      self.save_model(model)\n",
        "      self.count = 0\n",
        "      print(\"new best model is saved\")\n",
        "      self.best_model = copy.deepcopy(model)\n",
        "    else:\n",
        "      self.count += 1\n",
        "      if self.count == self.patience:\n",
        "        print('-'*50)\n",
        "        print(f\"training is over\")\n",
        "        print('-'*50)\n",
        "        self.stop = True\n",
        "\n",
        "  def save_mode(self, model):\n",
        "    torch.save(model.state_dict(), self.save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR2tReBBjqAv"
      },
      "source": [
        "def train_per_epoch(model, iterator, optimizer, criterion, clip, device):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "  for enu, (target_lang, source_lang) in enumerate(iterator):\n",
        "    source_lang, target_lang = source_lang.to(device), target_lang.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(source_lang, target_lang)\n",
        "    output = output[1:].view(-1, output.size()[-1]).to(device)\n",
        "    target_lang = target_lang[1:].view(-1)\n",
        "    loss = criterion(output, target_lang)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    print(f\"-----------------------the loss of {enu} iter is {loss:.3f}-----------------------\")\n",
        "    \n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss/len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "  model.eval()\n",
        "  epoch_loss = 0 \n",
        "  with torch.no_grad():\n",
        "    for enu, (target_lang, source_lang) in enumerate(iterator):\n",
        "      source_lang, target_lang = source_lang.to(device), target_lang.to(device)\n",
        "\n",
        "      output = model(source_lang, target_lang, teach_forcing_ratio = 0)\n",
        "\n",
        "      output = output[1:].view(-1, output.size()[-1]).to(device)\n",
        "      target_lang = target_lang[1:].view(-1)\n",
        "\n",
        "      loss = criterion(output, target_lang)\n",
        "        \n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "    return epoch_loss/len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def train(num_epochs, clip, model, train_iter, dev_iter, test_iter, criterion, device):\n",
        "  train_loss_log = []\n",
        "  valid_loss_log = []\n",
        "  early_stopper = early_stopping(patience = 10, save_path = \"./save_model\")\n",
        "  for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train_per_epoch(model, train_iter, optimizer, criterion, clip, device)\n",
        "    valid_loss = evaluate(model, dev_iter, criterion, device)\n",
        "    early_stopper(valid_loss, model)\n",
        "    train_loss_log.append(train_loss)\n",
        "    valid_loss_log.append(valid_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    if early_stopper.stop:\n",
        "      break\n",
        "  test_loss = evaluate(early_stopper.best_model, test_iter, criterion, device)\n",
        "\n",
        "  plt.plot(train_loss_log)\n",
        "  plt.plot(valid_loss_log)\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'dev'], loc='upper left')\n",
        "  plt.savefig(f\"./training_fig.png\", )\n",
        "\n",
        "  print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeEvVgFjQWEW"
      },
      "source": [
        "### 이미 1100 이터 정도 학습된 이후이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB8-8HqHo1SL",
        "outputId": "0c438c5b-c666-4935-ff76-4e9a09b3f78a"
      },
      "source": [
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "train(10, 1, model, train_iter, dev_iter, test_iter, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------the loss of 0 iter is 10.214-----------------------\n",
            "-----------------------the loss of 1 iter is 10.181-----------------------\n",
            "-----------------------the loss of 2 iter is 10.148-----------------------\n",
            "-----------------------the loss of 3 iter is 10.107-----------------------\n",
            "-----------------------the loss of 4 iter is 10.031-----------------------\n",
            "-----------------------the loss of 5 iter is 9.950-----------------------\n",
            "-----------------------the loss of 6 iter is 9.859-----------------------\n",
            "-----------------------the loss of 7 iter is 9.778-----------------------\n",
            "-----------------------the loss of 8 iter is 9.642-----------------------\n",
            "-----------------------the loss of 9 iter is 9.498-----------------------\n",
            "-----------------------the loss of 10 iter is 9.369-----------------------\n",
            "-----------------------the loss of 11 iter is 9.163-----------------------\n",
            "-----------------------the loss of 12 iter is 8.932-----------------------\n",
            "-----------------------the loss of 13 iter is 8.806-----------------------\n",
            "-----------------------the loss of 14 iter is 8.742-----------------------\n",
            "-----------------------the loss of 15 iter is 8.412-----------------------\n",
            "-----------------------the loss of 16 iter is 8.347-----------------------\n",
            "-----------------------the loss of 17 iter is 8.094-----------------------\n",
            "-----------------------the loss of 18 iter is 8.088-----------------------\n",
            "-----------------------the loss of 19 iter is 7.901-----------------------\n",
            "-----------------------the loss of 20 iter is 7.871-----------------------\n",
            "-----------------------the loss of 21 iter is 7.715-----------------------\n",
            "-----------------------the loss of 22 iter is 7.550-----------------------\n",
            "-----------------------the loss of 23 iter is 7.428-----------------------\n",
            "-----------------------the loss of 24 iter is 7.439-----------------------\n",
            "-----------------------the loss of 25 iter is 7.432-----------------------\n",
            "-----------------------the loss of 26 iter is 7.256-----------------------\n",
            "-----------------------the loss of 27 iter is 7.088-----------------------\n",
            "-----------------------the loss of 28 iter is 7.049-----------------------\n",
            "-----------------------the loss of 29 iter is 7.198-----------------------\n",
            "-----------------------the loss of 30 iter is 7.162-----------------------\n",
            "-----------------------the loss of 31 iter is 7.214-----------------------\n",
            "-----------------------the loss of 32 iter is 6.897-----------------------\n",
            "-----------------------the loss of 33 iter is 6.893-----------------------\n",
            "-----------------------the loss of 34 iter is 6.932-----------------------\n",
            "-----------------------the loss of 35 iter is 6.907-----------------------\n",
            "-----------------------the loss of 36 iter is 6.793-----------------------\n",
            "-----------------------the loss of 37 iter is 6.913-----------------------\n",
            "-----------------------the loss of 38 iter is 6.760-----------------------\n",
            "-----------------------the loss of 39 iter is 6.618-----------------------\n",
            "-----------------------the loss of 40 iter is 6.713-----------------------\n",
            "-----------------------the loss of 41 iter is 7.003-----------------------\n",
            "-----------------------the loss of 42 iter is 6.964-----------------------\n",
            "-----------------------the loss of 43 iter is 6.758-----------------------\n",
            "-----------------------the loss of 44 iter is 6.890-----------------------\n",
            "-----------------------the loss of 45 iter is 6.786-----------------------\n",
            "-----------------------the loss of 46 iter is 6.888-----------------------\n",
            "-----------------------the loss of 47 iter is 6.881-----------------------\n",
            "-----------------------the loss of 48 iter is 6.873-----------------------\n",
            "-----------------------the loss of 49 iter is 7.121-----------------------\n",
            "-----------------------the loss of 50 iter is 6.732-----------------------\n",
            "-----------------------the loss of 51 iter is 6.839-----------------------\n",
            "-----------------------the loss of 52 iter is 7.059-----------------------\n",
            "-----------------------the loss of 53 iter is 6.749-----------------------\n",
            "-----------------------the loss of 54 iter is 6.948-----------------------\n",
            "-----------------------the loss of 55 iter is 6.927-----------------------\n",
            "-----------------------the loss of 56 iter is 6.639-----------------------\n",
            "-----------------------the loss of 57 iter is 6.766-----------------------\n",
            "-----------------------the loss of 58 iter is 6.740-----------------------\n",
            "-----------------------the loss of 59 iter is 6.696-----------------------\n",
            "-----------------------the loss of 60 iter is 6.929-----------------------\n",
            "-----------------------the loss of 61 iter is 6.627-----------------------\n",
            "-----------------------the loss of 62 iter is 6.534-----------------------\n",
            "-----------------------the loss of 63 iter is 6.714-----------------------\n",
            "-----------------------the loss of 64 iter is 6.660-----------------------\n",
            "-----------------------the loss of 65 iter is 7.077-----------------------\n",
            "-----------------------the loss of 66 iter is 6.666-----------------------\n",
            "-----------------------the loss of 67 iter is 6.909-----------------------\n",
            "-----------------------the loss of 68 iter is 6.744-----------------------\n",
            "-----------------------the loss of 69 iter is 6.683-----------------------\n",
            "-----------------------the loss of 70 iter is 6.633-----------------------\n",
            "-----------------------the loss of 71 iter is 6.594-----------------------\n",
            "-----------------------the loss of 72 iter is 6.635-----------------------\n",
            "-----------------------the loss of 73 iter is 6.582-----------------------\n",
            "-----------------------the loss of 74 iter is 6.817-----------------------\n",
            "-----------------------the loss of 75 iter is 6.672-----------------------\n",
            "-----------------------the loss of 76 iter is 6.943-----------------------\n",
            "-----------------------the loss of 77 iter is 6.578-----------------------\n",
            "-----------------------the loss of 78 iter is 6.570-----------------------\n",
            "-----------------------the loss of 79 iter is 6.624-----------------------\n",
            "-----------------------the loss of 80 iter is 6.672-----------------------\n",
            "-----------------------the loss of 81 iter is 6.601-----------------------\n",
            "-----------------------the loss of 82 iter is 6.758-----------------------\n",
            "-----------------------the loss of 83 iter is 6.625-----------------------\n",
            "-----------------------the loss of 84 iter is 7.027-----------------------\n",
            "-----------------------the loss of 85 iter is 6.701-----------------------\n",
            "-----------------------the loss of 86 iter is 6.456-----------------------\n",
            "-----------------------the loss of 87 iter is 6.751-----------------------\n",
            "-----------------------the loss of 88 iter is 6.371-----------------------\n",
            "-----------------------the loss of 89 iter is 6.396-----------------------\n",
            "-----------------------the loss of 90 iter is 6.774-----------------------\n",
            "-----------------------the loss of 91 iter is 6.623-----------------------\n",
            "-----------------------the loss of 92 iter is 6.458-----------------------\n",
            "-----------------------the loss of 93 iter is 6.613-----------------------\n",
            "-----------------------the loss of 94 iter is 6.607-----------------------\n",
            "-----------------------the loss of 95 iter is 6.489-----------------------\n",
            "-----------------------the loss of 96 iter is 6.884-----------------------\n",
            "-----------------------the loss of 97 iter is 6.609-----------------------\n",
            "-----------------------the loss of 98 iter is 6.720-----------------------\n",
            "-----------------------the loss of 99 iter is 6.787-----------------------\n",
            "-----------------------the loss of 100 iter is 6.585-----------------------\n",
            "-----------------------the loss of 101 iter is 6.689-----------------------\n",
            "-----------------------the loss of 102 iter is 6.806-----------------------\n",
            "-----------------------the loss of 103 iter is 6.381-----------------------\n",
            "-----------------------the loss of 104 iter is 6.790-----------------------\n",
            "-----------------------the loss of 105 iter is 6.469-----------------------\n",
            "-----------------------the loss of 106 iter is 6.609-----------------------\n",
            "-----------------------the loss of 107 iter is 6.758-----------------------\n",
            "-----------------------the loss of 108 iter is 6.600-----------------------\n",
            "-----------------------the loss of 109 iter is 6.712-----------------------\n",
            "-----------------------the loss of 110 iter is 6.518-----------------------\n",
            "-----------------------the loss of 111 iter is 6.487-----------------------\n",
            "-----------------------the loss of 112 iter is 6.622-----------------------\n",
            "-----------------------the loss of 113 iter is 6.682-----------------------\n",
            "-----------------------the loss of 114 iter is 6.474-----------------------\n",
            "-----------------------the loss of 115 iter is 6.502-----------------------\n",
            "-----------------------the loss of 116 iter is 6.573-----------------------\n",
            "-----------------------the loss of 117 iter is 6.727-----------------------\n",
            "-----------------------the loss of 118 iter is 6.560-----------------------\n",
            "-----------------------the loss of 119 iter is 6.416-----------------------\n",
            "-----------------------the loss of 120 iter is 6.391-----------------------\n",
            "-----------------------the loss of 121 iter is 6.482-----------------------\n",
            "-----------------------the loss of 122 iter is 6.822-----------------------\n",
            "-----------------------the loss of 123 iter is 6.427-----------------------\n",
            "-----------------------the loss of 124 iter is 6.719-----------------------\n",
            "-----------------------the loss of 125 iter is 6.652-----------------------\n",
            "-----------------------the loss of 126 iter is 6.578-----------------------\n",
            "-----------------------the loss of 127 iter is 6.557-----------------------\n",
            "-----------------------the loss of 128 iter is 6.333-----------------------\n",
            "-----------------------the loss of 129 iter is 6.752-----------------------\n",
            "-----------------------the loss of 130 iter is 6.381-----------------------\n",
            "-----------------------the loss of 131 iter is 6.475-----------------------\n",
            "-----------------------the loss of 132 iter is 6.788-----------------------\n",
            "-----------------------the loss of 133 iter is 6.614-----------------------\n",
            "-----------------------the loss of 134 iter is 6.378-----------------------\n",
            "-----------------------the loss of 135 iter is 6.591-----------------------\n",
            "-----------------------the loss of 136 iter is 6.563-----------------------\n",
            "-----------------------the loss of 137 iter is 6.529-----------------------\n",
            "-----------------------the loss of 138 iter is 6.454-----------------------\n",
            "-----------------------the loss of 139 iter is 6.565-----------------------\n",
            "-----------------------the loss of 140 iter is 6.576-----------------------\n",
            "-----------------------the loss of 141 iter is 6.362-----------------------\n",
            "-----------------------the loss of 142 iter is 6.628-----------------------\n",
            "-----------------------the loss of 143 iter is 6.200-----------------------\n",
            "-----------------------the loss of 144 iter is 6.632-----------------------\n",
            "-----------------------the loss of 145 iter is 6.466-----------------------\n",
            "-----------------------the loss of 146 iter is 6.746-----------------------\n",
            "-----------------------the loss of 147 iter is 6.458-----------------------\n",
            "-----------------------the loss of 148 iter is 6.435-----------------------\n",
            "-----------------------the loss of 149 iter is 6.635-----------------------\n",
            "-----------------------the loss of 150 iter is 6.376-----------------------\n",
            "-----------------------the loss of 151 iter is 6.176-----------------------\n",
            "-----------------------the loss of 152 iter is 6.410-----------------------\n",
            "-----------------------the loss of 153 iter is 6.528-----------------------\n",
            "-----------------------the loss of 154 iter is 6.452-----------------------\n",
            "-----------------------the loss of 155 iter is 6.373-----------------------\n",
            "-----------------------the loss of 156 iter is 6.808-----------------------\n",
            "-----------------------the loss of 157 iter is 6.409-----------------------\n",
            "-----------------------the loss of 158 iter is 6.497-----------------------\n",
            "-----------------------the loss of 159 iter is 6.433-----------------------\n",
            "-----------------------the loss of 160 iter is 6.630-----------------------\n",
            "-----------------------the loss of 161 iter is 6.502-----------------------\n",
            "-----------------------the loss of 162 iter is 6.495-----------------------\n",
            "-----------------------the loss of 163 iter is 6.193-----------------------\n",
            "-----------------------the loss of 164 iter is 6.329-----------------------\n",
            "-----------------------the loss of 165 iter is 6.144-----------------------\n",
            "-----------------------the loss of 166 iter is 6.604-----------------------\n",
            "-----------------------the loss of 167 iter is 6.527-----------------------\n",
            "-----------------------the loss of 168 iter is 6.412-----------------------\n",
            "-----------------------the loss of 169 iter is 6.570-----------------------\n",
            "-----------------------the loss of 170 iter is 6.306-----------------------\n",
            "-----------------------the loss of 171 iter is 6.722-----------------------\n",
            "-----------------------the loss of 172 iter is 6.808-----------------------\n",
            "-----------------------the loss of 173 iter is 6.623-----------------------\n",
            "-----------------------the loss of 174 iter is 6.283-----------------------\n",
            "-----------------------the loss of 175 iter is 6.525-----------------------\n",
            "-----------------------the loss of 176 iter is 6.226-----------------------\n",
            "-----------------------the loss of 177 iter is 6.476-----------------------\n",
            "-----------------------the loss of 178 iter is 6.223-----------------------\n",
            "-----------------------the loss of 179 iter is 6.624-----------------------\n",
            "-----------------------the loss of 180 iter is 6.436-----------------------\n",
            "-----------------------the loss of 181 iter is 6.716-----------------------\n",
            "-----------------------the loss of 182 iter is 6.550-----------------------\n",
            "-----------------------the loss of 183 iter is 6.366-----------------------\n",
            "-----------------------the loss of 184 iter is 6.228-----------------------\n",
            "-----------------------the loss of 185 iter is 6.211-----------------------\n",
            "-----------------------the loss of 186 iter is 6.296-----------------------\n",
            "-----------------------the loss of 187 iter is 6.671-----------------------\n",
            "-----------------------the loss of 188 iter is 6.345-----------------------\n",
            "-----------------------the loss of 189 iter is 6.395-----------------------\n",
            "-----------------------the loss of 190 iter is 6.601-----------------------\n",
            "-----------------------the loss of 191 iter is 6.247-----------------------\n",
            "-----------------------the loss of 192 iter is 6.017-----------------------\n",
            "-----------------------the loss of 193 iter is 6.417-----------------------\n",
            "-----------------------the loss of 194 iter is 6.706-----------------------\n",
            "-----------------------the loss of 195 iter is 6.483-----------------------\n",
            "-----------------------the loss of 196 iter is 6.450-----------------------\n",
            "-----------------------the loss of 197 iter is 6.548-----------------------\n",
            "-----------------------the loss of 198 iter is 6.429-----------------------\n",
            "-----------------------the loss of 199 iter is 6.304-----------------------\n",
            "-----------------------the loss of 200 iter is 6.299-----------------------\n",
            "-----------------------the loss of 201 iter is 6.421-----------------------\n",
            "-----------------------the loss of 202 iter is 6.036-----------------------\n",
            "-----------------------the loss of 203 iter is 6.511-----------------------\n",
            "-----------------------the loss of 204 iter is 6.543-----------------------\n",
            "-----------------------the loss of 205 iter is 6.493-----------------------\n",
            "-----------------------the loss of 206 iter is 6.130-----------------------\n",
            "-----------------------the loss of 207 iter is 6.086-----------------------\n",
            "-----------------------the loss of 208 iter is 6.361-----------------------\n",
            "-----------------------the loss of 209 iter is 6.574-----------------------\n",
            "-----------------------the loss of 210 iter is 6.017-----------------------\n",
            "-----------------------the loss of 211 iter is 6.480-----------------------\n",
            "-----------------------the loss of 212 iter is 6.461-----------------------\n",
            "-----------------------the loss of 213 iter is 6.334-----------------------\n",
            "-----------------------the loss of 214 iter is 6.171-----------------------\n",
            "-----------------------the loss of 215 iter is 6.420-----------------------\n",
            "-----------------------the loss of 216 iter is 6.556-----------------------\n",
            "-----------------------the loss of 217 iter is 6.560-----------------------\n",
            "-----------------------the loss of 218 iter is 6.335-----------------------\n",
            "-----------------------the loss of 219 iter is 6.568-----------------------\n",
            "-----------------------the loss of 220 iter is 6.613-----------------------\n",
            "-----------------------the loss of 221 iter is 6.248-----------------------\n",
            "-----------------------the loss of 222 iter is 6.066-----------------------\n",
            "-----------------------the loss of 223 iter is 6.573-----------------------\n",
            "-----------------------the loss of 224 iter is 6.346-----------------------\n",
            "-----------------------the loss of 225 iter is 6.353-----------------------\n",
            "-----------------------the loss of 226 iter is 6.134-----------------------\n",
            "-----------------------the loss of 227 iter is 6.297-----------------------\n",
            "-----------------------the loss of 228 iter is 6.463-----------------------\n",
            "-----------------------the loss of 229 iter is 6.229-----------------------\n",
            "-----------------------the loss of 230 iter is 6.277-----------------------\n",
            "-----------------------the loss of 231 iter is 6.520-----------------------\n",
            "-----------------------the loss of 232 iter is 6.508-----------------------\n",
            "-----------------------the loss of 233 iter is 6.341-----------------------\n",
            "-----------------------the loss of 234 iter is 6.472-----------------------\n",
            "-----------------------the loss of 235 iter is 6.127-----------------------\n",
            "-----------------------the loss of 236 iter is 6.625-----------------------\n",
            "-----------------------the loss of 237 iter is 6.338-----------------------\n",
            "-----------------------the loss of 238 iter is 5.998-----------------------\n",
            "-----------------------the loss of 239 iter is 6.120-----------------------\n",
            "-----------------------the loss of 240 iter is 6.193-----------------------\n",
            "-----------------------the loss of 241 iter is 6.913-----------------------\n",
            "-----------------------the loss of 242 iter is 6.333-----------------------\n",
            "-----------------------the loss of 243 iter is 6.303-----------------------\n",
            "-----------------------the loss of 244 iter is 6.032-----------------------\n",
            "-----------------------the loss of 245 iter is 6.126-----------------------\n",
            "-----------------------the loss of 246 iter is 6.359-----------------------\n",
            "-----------------------the loss of 247 iter is 6.230-----------------------\n",
            "-----------------------the loss of 248 iter is 6.802-----------------------\n",
            "-----------------------the loss of 249 iter is 6.213-----------------------\n",
            "-----------------------the loss of 250 iter is 6.207-----------------------\n",
            "-----------------------the loss of 251 iter is 5.958-----------------------\n",
            "-----------------------the loss of 252 iter is 6.463-----------------------\n",
            "-----------------------the loss of 253 iter is 6.550-----------------------\n",
            "-----------------------the loss of 254 iter is 6.229-----------------------\n",
            "-----------------------the loss of 255 iter is 6.297-----------------------\n",
            "-----------------------the loss of 256 iter is 6.061-----------------------\n",
            "-----------------------the loss of 257 iter is 6.349-----------------------\n",
            "-----------------------the loss of 258 iter is 6.102-----------------------\n",
            "-----------------------the loss of 259 iter is 6.312-----------------------\n",
            "-----------------------the loss of 260 iter is 6.231-----------------------\n",
            "-----------------------the loss of 261 iter is 6.150-----------------------\n",
            "-----------------------the loss of 262 iter is 6.495-----------------------\n",
            "-----------------------the loss of 263 iter is 6.349-----------------------\n",
            "-----------------------the loss of 264 iter is 6.134-----------------------\n",
            "-----------------------the loss of 265 iter is 6.290-----------------------\n",
            "-----------------------the loss of 266 iter is 6.140-----------------------\n",
            "-----------------------the loss of 267 iter is 6.107-----------------------\n",
            "-----------------------the loss of 268 iter is 6.250-----------------------\n",
            "-----------------------the loss of 269 iter is 6.266-----------------------\n",
            "-----------------------the loss of 270 iter is 6.414-----------------------\n",
            "-----------------------the loss of 271 iter is 6.505-----------------------\n",
            "-----------------------the loss of 272 iter is 5.870-----------------------\n",
            "-----------------------the loss of 273 iter is 6.233-----------------------\n",
            "-----------------------the loss of 274 iter is 6.234-----------------------\n",
            "-----------------------the loss of 275 iter is 6.327-----------------------\n",
            "-----------------------the loss of 276 iter is 6.250-----------------------\n",
            "-----------------------the loss of 277 iter is 5.938-----------------------\n",
            "-----------------------the loss of 278 iter is 6.221-----------------------\n",
            "-----------------------the loss of 279 iter is 6.122-----------------------\n",
            "-----------------------the loss of 280 iter is 6.177-----------------------\n",
            "-----------------------the loss of 281 iter is 6.342-----------------------\n",
            "-----------------------the loss of 282 iter is 6.293-----------------------\n",
            "-----------------------the loss of 283 iter is 6.216-----------------------\n",
            "-----------------------the loss of 284 iter is 5.790-----------------------\n",
            "-----------------------the loss of 285 iter is 6.287-----------------------\n",
            "-----------------------the loss of 286 iter is 6.357-----------------------\n",
            "-----------------------the loss of 287 iter is 6.326-----------------------\n",
            "-----------------------the loss of 288 iter is 6.179-----------------------\n",
            "-----------------------the loss of 289 iter is 6.269-----------------------\n",
            "-----------------------the loss of 290 iter is 6.437-----------------------\n",
            "-----------------------the loss of 291 iter is 6.197-----------------------\n",
            "-----------------------the loss of 292 iter is 6.401-----------------------\n",
            "-----------------------the loss of 293 iter is 6.198-----------------------\n",
            "-----------------------the loss of 294 iter is 6.028-----------------------\n",
            "-----------------------the loss of 295 iter is 6.089-----------------------\n",
            "-----------------------the loss of 296 iter is 5.822-----------------------\n",
            "-----------------------the loss of 297 iter is 6.229-----------------------\n",
            "-----------------------the loss of 298 iter is 6.296-----------------------\n",
            "-----------------------the loss of 299 iter is 6.136-----------------------\n",
            "-----------------------the loss of 300 iter is 6.314-----------------------\n",
            "-----------------------the loss of 301 iter is 6.409-----------------------\n",
            "-----------------------the loss of 302 iter is 6.260-----------------------\n",
            "-----------------------the loss of 303 iter is 6.213-----------------------\n",
            "-----------------------the loss of 304 iter is 6.293-----------------------\n",
            "-----------------------the loss of 305 iter is 6.079-----------------------\n",
            "-----------------------the loss of 306 iter is 5.918-----------------------\n",
            "-----------------------the loss of 307 iter is 6.152-----------------------\n",
            "-----------------------the loss of 308 iter is 6.478-----------------------\n",
            "-----------------------the loss of 309 iter is 6.251-----------------------\n",
            "-----------------------the loss of 310 iter is 6.129-----------------------\n",
            "-----------------------the loss of 311 iter is 6.049-----------------------\n",
            "-----------------------the loss of 312 iter is 6.090-----------------------\n",
            "-----------------------the loss of 313 iter is 6.601-----------------------\n",
            "-----------------------the loss of 314 iter is 6.080-----------------------\n",
            "-----------------------the loss of 315 iter is 6.031-----------------------\n",
            "-----------------------the loss of 316 iter is 6.259-----------------------\n",
            "-----------------------the loss of 317 iter is 6.253-----------------------\n",
            "-----------------------the loss of 318 iter is 6.167-----------------------\n",
            "-----------------------the loss of 319 iter is 6.241-----------------------\n",
            "-----------------------the loss of 320 iter is 5.953-----------------------\n",
            "-----------------------the loss of 321 iter is 6.107-----------------------\n",
            "-----------------------the loss of 322 iter is 6.182-----------------------\n",
            "-----------------------the loss of 323 iter is 5.954-----------------------\n",
            "-----------------------the loss of 324 iter is 6.106-----------------------\n",
            "-----------------------the loss of 325 iter is 6.196-----------------------\n",
            "-----------------------the loss of 326 iter is 6.129-----------------------\n",
            "-----------------------the loss of 327 iter is 5.889-----------------------\n",
            "-----------------------the loss of 328 iter is 6.467-----------------------\n",
            "-----------------------the loss of 329 iter is 5.977-----------------------\n",
            "-----------------------the loss of 330 iter is 5.964-----------------------\n",
            "-----------------------the loss of 331 iter is 6.272-----------------------\n",
            "-----------------------the loss of 332 iter is 5.794-----------------------\n",
            "-----------------------the loss of 333 iter is 5.594-----------------------\n",
            "-----------------------the loss of 334 iter is 5.943-----------------------\n",
            "-----------------------the loss of 335 iter is 6.109-----------------------\n",
            "-----------------------the loss of 336 iter is 6.313-----------------------\n",
            "-----------------------the loss of 337 iter is 6.603-----------------------\n",
            "-----------------------the loss of 338 iter is 6.355-----------------------\n",
            "-----------------------the loss of 339 iter is 6.410-----------------------\n",
            "-----------------------the loss of 340 iter is 6.026-----------------------\n",
            "-----------------------the loss of 341 iter is 6.256-----------------------\n",
            "-----------------------the loss of 342 iter is 6.199-----------------------\n",
            "-----------------------the loss of 343 iter is 6.067-----------------------\n",
            "-----------------------the loss of 344 iter is 6.033-----------------------\n",
            "-----------------------the loss of 345 iter is 6.190-----------------------\n",
            "-----------------------the loss of 346 iter is 5.821-----------------------\n",
            "-----------------------the loss of 347 iter is 6.005-----------------------\n",
            "-----------------------the loss of 348 iter is 6.255-----------------------\n",
            "-----------------------the loss of 349 iter is 5.922-----------------------\n",
            "-----------------------the loss of 350 iter is 6.479-----------------------\n",
            "-----------------------the loss of 351 iter is 6.471-----------------------\n",
            "-----------------------the loss of 352 iter is 5.881-----------------------\n",
            "-----------------------the loss of 353 iter is 5.905-----------------------\n",
            "-----------------------the loss of 354 iter is 6.157-----------------------\n",
            "-----------------------the loss of 355 iter is 5.872-----------------------\n",
            "-----------------------the loss of 356 iter is 6.066-----------------------\n",
            "-----------------------the loss of 357 iter is 6.183-----------------------\n",
            "-----------------------the loss of 358 iter is 6.519-----------------------\n",
            "-----------------------the loss of 359 iter is 5.807-----------------------\n",
            "-----------------------the loss of 360 iter is 6.231-----------------------\n",
            "-----------------------the loss of 361 iter is 6.172-----------------------\n",
            "-----------------------the loss of 362 iter is 6.148-----------------------\n",
            "-----------------------the loss of 363 iter is 6.190-----------------------\n",
            "-----------------------the loss of 364 iter is 5.839-----------------------\n",
            "-----------------------the loss of 365 iter is 5.883-----------------------\n",
            "-----------------------the loss of 366 iter is 5.801-----------------------\n",
            "-----------------------the loss of 367 iter is 6.007-----------------------\n",
            "-----------------------the loss of 368 iter is 6.261-----------------------\n",
            "-----------------------the loss of 369 iter is 6.371-----------------------\n",
            "-----------------------the loss of 370 iter is 6.399-----------------------\n",
            "-----------------------the loss of 371 iter is 5.827-----------------------\n",
            "-----------------------the loss of 372 iter is 5.999-----------------------\n",
            "-----------------------the loss of 373 iter is 6.068-----------------------\n",
            "-----------------------the loss of 374 iter is 5.927-----------------------\n",
            "-----------------------the loss of 375 iter is 6.103-----------------------\n",
            "-----------------------the loss of 376 iter is 5.993-----------------------\n",
            "-----------------------the loss of 377 iter is 5.868-----------------------\n",
            "-----------------------the loss of 378 iter is 5.846-----------------------\n",
            "-----------------------the loss of 379 iter is 6.091-----------------------\n",
            "-----------------------the loss of 380 iter is 5.967-----------------------\n",
            "-----------------------the loss of 381 iter is 5.863-----------------------\n",
            "-----------------------the loss of 382 iter is 5.958-----------------------\n",
            "-----------------------the loss of 383 iter is 6.401-----------------------\n",
            "-----------------------the loss of 384 iter is 5.614-----------------------\n",
            "-----------------------the loss of 385 iter is 6.348-----------------------\n",
            "-----------------------the loss of 386 iter is 6.278-----------------------\n",
            "-----------------------the loss of 387 iter is 5.866-----------------------\n",
            "-----------------------the loss of 388 iter is 6.347-----------------------\n",
            "-----------------------the loss of 389 iter is 6.181-----------------------\n",
            "-----------------------the loss of 390 iter is 5.993-----------------------\n",
            "-----------------------the loss of 391 iter is 6.099-----------------------\n",
            "-----------------------the loss of 392 iter is 5.953-----------------------\n",
            "-----------------------the loss of 393 iter is 5.866-----------------------\n",
            "-----------------------the loss of 394 iter is 6.313-----------------------\n",
            "-----------------------the loss of 395 iter is 6.472-----------------------\n",
            "-----------------------the loss of 396 iter is 5.774-----------------------\n",
            "-----------------------the loss of 397 iter is 5.812-----------------------\n",
            "-----------------------the loss of 398 iter is 6.060-----------------------\n",
            "-----------------------the loss of 399 iter is 5.802-----------------------\n",
            "-----------------------the loss of 400 iter is 6.083-----------------------\n",
            "-----------------------the loss of 401 iter is 5.862-----------------------\n",
            "-----------------------the loss of 402 iter is 6.071-----------------------\n",
            "-----------------------the loss of 403 iter is 6.067-----------------------\n",
            "-----------------------the loss of 404 iter is 6.316-----------------------\n",
            "-----------------------the loss of 405 iter is 6.248-----------------------\n",
            "-----------------------the loss of 406 iter is 6.051-----------------------\n",
            "-----------------------the loss of 407 iter is 5.807-----------------------\n",
            "-----------------------the loss of 408 iter is 6.050-----------------------\n",
            "-----------------------the loss of 409 iter is 5.981-----------------------\n",
            "-----------------------the loss of 410 iter is 5.974-----------------------\n",
            "-----------------------the loss of 411 iter is 6.110-----------------------\n",
            "-----------------------the loss of 412 iter is 6.150-----------------------\n",
            "-----------------------the loss of 413 iter is 6.323-----------------------\n",
            "-----------------------the loss of 414 iter is 6.309-----------------------\n",
            "-----------------------the loss of 415 iter is 5.856-----------------------\n",
            "-----------------------the loss of 416 iter is 6.124-----------------------\n",
            "-----------------------the loss of 417 iter is 6.063-----------------------\n",
            "-----------------------the loss of 418 iter is 5.877-----------------------\n",
            "-----------------------the loss of 419 iter is 6.087-----------------------\n",
            "-----------------------the loss of 420 iter is 6.185-----------------------\n",
            "-----------------------the loss of 421 iter is 5.718-----------------------\n",
            "-----------------------the loss of 422 iter is 6.040-----------------------\n",
            "-----------------------the loss of 423 iter is 5.977-----------------------\n",
            "-----------------------the loss of 424 iter is 6.202-----------------------\n",
            "-----------------------the loss of 425 iter is 6.016-----------------------\n",
            "-----------------------the loss of 426 iter is 5.925-----------------------\n",
            "-----------------------the loss of 427 iter is 6.026-----------------------\n",
            "-----------------------the loss of 428 iter is 6.317-----------------------\n",
            "-----------------------the loss of 429 iter is 5.737-----------------------\n",
            "-----------------------the loss of 430 iter is 6.088-----------------------\n",
            "-----------------------the loss of 431 iter is 6.071-----------------------\n",
            "-----------------------the loss of 432 iter is 6.269-----------------------\n",
            "-----------------------the loss of 433 iter is 6.223-----------------------\n",
            "-----------------------the loss of 434 iter is 5.598-----------------------\n",
            "-----------------------the loss of 435 iter is 5.882-----------------------\n",
            "-----------------------the loss of 436 iter is 5.958-----------------------\n",
            "-----------------------the loss of 437 iter is 6.072-----------------------\n",
            "-----------------------the loss of 438 iter is 5.978-----------------------\n",
            "-----------------------the loss of 439 iter is 5.995-----------------------\n",
            "-----------------------the loss of 440 iter is 6.097-----------------------\n",
            "-----------------------the loss of 441 iter is 6.065-----------------------\n",
            "-----------------------the loss of 442 iter is 5.888-----------------------\n",
            "-----------------------the loss of 443 iter is 6.184-----------------------\n",
            "-----------------------the loss of 444 iter is 6.197-----------------------\n",
            "-----------------------the loss of 445 iter is 5.867-----------------------\n",
            "-----------------------the loss of 446 iter is 5.943-----------------------\n",
            "-----------------------the loss of 447 iter is 5.970-----------------------\n",
            "-----------------------the loss of 448 iter is 6.328-----------------------\n",
            "-----------------------the loss of 449 iter is 5.868-----------------------\n",
            "-----------------------the loss of 450 iter is 5.994-----------------------\n",
            "-----------------------the loss of 451 iter is 5.835-----------------------\n",
            "-----------------------the loss of 452 iter is 5.892-----------------------\n",
            "-----------------------the loss of 453 iter is 6.159-----------------------\n",
            "-----------------------the loss of 454 iter is 6.307-----------------------\n",
            "-----------------------the loss of 455 iter is 6.010-----------------------\n",
            "-----------------------the loss of 456 iter is 5.859-----------------------\n",
            "-----------------------the loss of 457 iter is 6.122-----------------------\n",
            "-----------------------the loss of 458 iter is 6.012-----------------------\n",
            "-----------------------the loss of 459 iter is 6.220-----------------------\n",
            "-----------------------the loss of 460 iter is 5.852-----------------------\n",
            "-----------------------the loss of 461 iter is 5.571-----------------------\n",
            "-----------------------the loss of 462 iter is 5.895-----------------------\n",
            "-----------------------the loss of 463 iter is 6.340-----------------------\n",
            "-----------------------the loss of 464 iter is 6.051-----------------------\n",
            "-----------------------the loss of 465 iter is 6.102-----------------------\n",
            "-----------------------the loss of 466 iter is 5.961-----------------------\n",
            "-----------------------the loss of 467 iter is 5.976-----------------------\n",
            "-----------------------the loss of 468 iter is 5.978-----------------------\n",
            "-----------------------the loss of 469 iter is 6.123-----------------------\n",
            "-----------------------the loss of 470 iter is 6.290-----------------------\n",
            "-----------------------the loss of 471 iter is 5.964-----------------------\n",
            "-----------------------the loss of 472 iter is 6.023-----------------------\n",
            "-----------------------the loss of 473 iter is 6.288-----------------------\n",
            "-----------------------the loss of 474 iter is 6.070-----------------------\n",
            "-----------------------the loss of 475 iter is 5.921-----------------------\n",
            "-----------------------the loss of 476 iter is 6.014-----------------------\n",
            "-----------------------the loss of 477 iter is 6.340-----------------------\n",
            "-----------------------the loss of 478 iter is 5.814-----------------------\n",
            "-----------------------the loss of 479 iter is 5.821-----------------------\n",
            "-----------------------the loss of 480 iter is 5.853-----------------------\n",
            "-----------------------the loss of 481 iter is 6.057-----------------------\n",
            "-----------------------the loss of 482 iter is 6.007-----------------------\n",
            "-----------------------the loss of 483 iter is 5.997-----------------------\n",
            "-----------------------the loss of 484 iter is 5.811-----------------------\n",
            "-----------------------the loss of 485 iter is 6.099-----------------------\n",
            "-----------------------the loss of 486 iter is 6.187-----------------------\n",
            "-----------------------the loss of 487 iter is 6.027-----------------------\n",
            "-----------------------the loss of 488 iter is 6.116-----------------------\n",
            "-----------------------the loss of 489 iter is 6.229-----------------------\n",
            "-----------------------the loss of 490 iter is 6.044-----------------------\n",
            "-----------------------the loss of 491 iter is 5.973-----------------------\n",
            "-----------------------the loss of 492 iter is 5.962-----------------------\n",
            "-----------------------the loss of 493 iter is 6.085-----------------------\n",
            "-----------------------the loss of 494 iter is 5.793-----------------------\n",
            "-----------------------the loss of 495 iter is 6.087-----------------------\n",
            "-----------------------the loss of 496 iter is 5.316-----------------------\n",
            "-----------------------the loss of 497 iter is 5.850-----------------------\n",
            "-----------------------the loss of 498 iter is 5.937-----------------------\n",
            "-----------------------the loss of 499 iter is 6.095-----------------------\n",
            "-----------------------the loss of 500 iter is 5.664-----------------------\n",
            "-----------------------the loss of 501 iter is 6.407-----------------------\n",
            "-----------------------the loss of 502 iter is 6.188-----------------------\n",
            "-----------------------the loss of 503 iter is 5.518-----------------------\n",
            "-----------------------the loss of 504 iter is 6.235-----------------------\n",
            "-----------------------the loss of 505 iter is 6.208-----------------------\n",
            "-----------------------the loss of 506 iter is 5.940-----------------------\n",
            "-----------------------the loss of 507 iter is 5.801-----------------------\n",
            "-----------------------the loss of 508 iter is 6.400-----------------------\n",
            "-----------------------the loss of 509 iter is 6.102-----------------------\n",
            "-----------------------the loss of 510 iter is 5.969-----------------------\n",
            "-----------------------the loss of 511 iter is 5.819-----------------------\n",
            "-----------------------the loss of 512 iter is 6.212-----------------------\n",
            "-----------------------the loss of 513 iter is 6.103-----------------------\n",
            "-----------------------the loss of 514 iter is 5.937-----------------------\n",
            "-----------------------the loss of 515 iter is 5.994-----------------------\n",
            "-----------------------the loss of 516 iter is 6.009-----------------------\n",
            "-----------------------the loss of 517 iter is 5.773-----------------------\n",
            "-----------------------the loss of 518 iter is 6.048-----------------------\n",
            "-----------------------the loss of 519 iter is 6.016-----------------------\n",
            "-----------------------the loss of 520 iter is 5.613-----------------------\n",
            "-----------------------the loss of 521 iter is 5.372-----------------------\n",
            "-----------------------the loss of 522 iter is 6.007-----------------------\n",
            "-----------------------the loss of 523 iter is 6.097-----------------------\n",
            "-----------------------the loss of 524 iter is 5.981-----------------------\n",
            "-----------------------the loss of 525 iter is 5.844-----------------------\n",
            "-----------------------the loss of 526 iter is 5.759-----------------------\n",
            "-----------------------the loss of 527 iter is 6.043-----------------------\n",
            "-----------------------the loss of 528 iter is 6.093-----------------------\n",
            "-----------------------the loss of 529 iter is 6.204-----------------------\n",
            "-----------------------the loss of 530 iter is 5.766-----------------------\n",
            "-----------------------the loss of 531 iter is 6.066-----------------------\n",
            "-----------------------the loss of 532 iter is 6.100-----------------------\n",
            "-----------------------the loss of 533 iter is 6.057-----------------------\n",
            "-----------------------the loss of 534 iter is 5.773-----------------------\n",
            "-----------------------the loss of 535 iter is 6.108-----------------------\n",
            "-----------------------the loss of 536 iter is 5.991-----------------------\n",
            "-----------------------the loss of 537 iter is 5.918-----------------------\n",
            "-----------------------the loss of 538 iter is 5.655-----------------------\n",
            "-----------------------the loss of 539 iter is 5.432-----------------------\n",
            "-----------------------the loss of 540 iter is 6.037-----------------------\n",
            "-----------------------the loss of 541 iter is 5.688-----------------------\n",
            "-----------------------the loss of 542 iter is 5.736-----------------------\n",
            "-----------------------the loss of 543 iter is 6.019-----------------------\n",
            "-----------------------the loss of 544 iter is 5.846-----------------------\n",
            "-----------------------the loss of 545 iter is 5.745-----------------------\n",
            "-----------------------the loss of 546 iter is 5.698-----------------------\n",
            "-----------------------the loss of 547 iter is 5.940-----------------------\n",
            "-----------------------the loss of 548 iter is 6.312-----------------------\n",
            "-----------------------the loss of 549 iter is 5.629-----------------------\n",
            "-----------------------the loss of 550 iter is 5.774-----------------------\n",
            "-----------------------the loss of 551 iter is 6.033-----------------------\n",
            "-----------------------the loss of 552 iter is 5.752-----------------------\n",
            "-----------------------the loss of 553 iter is 6.021-----------------------\n",
            "-----------------------the loss of 554 iter is 5.747-----------------------\n",
            "-----------------------the loss of 555 iter is 5.917-----------------------\n",
            "-----------------------the loss of 556 iter is 5.957-----------------------\n",
            "-----------------------the loss of 557 iter is 5.968-----------------------\n",
            "-----------------------the loss of 558 iter is 5.656-----------------------\n",
            "-----------------------the loss of 559 iter is 5.575-----------------------\n",
            "-----------------------the loss of 560 iter is 6.152-----------------------\n",
            "-----------------------the loss of 561 iter is 5.728-----------------------\n",
            "-----------------------the loss of 562 iter is 5.884-----------------------\n",
            "-----------------------the loss of 563 iter is 6.194-----------------------\n",
            "-----------------------the loss of 564 iter is 5.765-----------------------\n",
            "-----------------------the loss of 565 iter is 5.945-----------------------\n",
            "-----------------------the loss of 566 iter is 6.060-----------------------\n",
            "-----------------------the loss of 567 iter is 5.824-----------------------\n",
            "-----------------------the loss of 568 iter is 6.011-----------------------\n",
            "-----------------------the loss of 569 iter is 5.937-----------------------\n",
            "-----------------------the loss of 570 iter is 6.236-----------------------\n",
            "-----------------------the loss of 571 iter is 5.666-----------------------\n",
            "-----------------------the loss of 572 iter is 6.190-----------------------\n",
            "-----------------------the loss of 573 iter is 5.474-----------------------\n",
            "-----------------------the loss of 574 iter is 5.996-----------------------\n",
            "-----------------------the loss of 575 iter is 6.057-----------------------\n",
            "-----------------------the loss of 576 iter is 5.795-----------------------\n",
            "-----------------------the loss of 577 iter is 5.631-----------------------\n",
            "-----------------------the loss of 578 iter is 5.952-----------------------\n",
            "-----------------------the loss of 579 iter is 5.530-----------------------\n",
            "-----------------------the loss of 580 iter is 5.734-----------------------\n",
            "-----------------------the loss of 581 iter is 6.037-----------------------\n",
            "-----------------------the loss of 582 iter is 6.047-----------------------\n",
            "-----------------------the loss of 583 iter is 5.536-----------------------\n",
            "-----------------------the loss of 584 iter is 5.926-----------------------\n",
            "-----------------------the loss of 585 iter is 5.457-----------------------\n",
            "-----------------------the loss of 586 iter is 6.019-----------------------\n",
            "-----------------------the loss of 587 iter is 5.667-----------------------\n",
            "-----------------------the loss of 588 iter is 5.639-----------------------\n",
            "-----------------------the loss of 589 iter is 5.783-----------------------\n",
            "-----------------------the loss of 590 iter is 5.832-----------------------\n",
            "-----------------------the loss of 591 iter is 5.582-----------------------\n",
            "-----------------------the loss of 592 iter is 5.773-----------------------\n",
            "-----------------------the loss of 593 iter is 5.604-----------------------\n",
            "-----------------------the loss of 594 iter is 6.047-----------------------\n",
            "-----------------------the loss of 595 iter is 6.000-----------------------\n",
            "-----------------------the loss of 596 iter is 5.811-----------------------\n",
            "-----------------------the loss of 597 iter is 5.976-----------------------\n",
            "-----------------------the loss of 598 iter is 5.790-----------------------\n",
            "-----------------------the loss of 599 iter is 6.038-----------------------\n",
            "-----------------------the loss of 600 iter is 5.631-----------------------\n",
            "-----------------------the loss of 601 iter is 5.650-----------------------\n",
            "-----------------------the loss of 602 iter is 5.590-----------------------\n",
            "-----------------------the loss of 603 iter is 5.898-----------------------\n",
            "-----------------------the loss of 604 iter is 6.146-----------------------\n",
            "-----------------------the loss of 605 iter is 5.890-----------------------\n",
            "-----------------------the loss of 606 iter is 5.731-----------------------\n",
            "-----------------------the loss of 607 iter is 5.577-----------------------\n",
            "-----------------------the loss of 608 iter is 6.076-----------------------\n",
            "-----------------------the loss of 609 iter is 5.877-----------------------\n",
            "-----------------------the loss of 610 iter is 5.558-----------------------\n",
            "-----------------------the loss of 611 iter is 5.305-----------------------\n",
            "-----------------------the loss of 612 iter is 5.935-----------------------\n",
            "-----------------------the loss of 613 iter is 5.975-----------------------\n",
            "-----------------------the loss of 614 iter is 5.626-----------------------\n",
            "-----------------------the loss of 615 iter is 5.893-----------------------\n",
            "-----------------------the loss of 616 iter is 6.070-----------------------\n",
            "-----------------------the loss of 617 iter is 6.000-----------------------\n",
            "-----------------------the loss of 618 iter is 5.609-----------------------\n",
            "-----------------------the loss of 619 iter is 6.069-----------------------\n",
            "-----------------------the loss of 620 iter is 5.984-----------------------\n",
            "-----------------------the loss of 621 iter is 5.570-----------------------\n",
            "-----------------------the loss of 622 iter is 5.780-----------------------\n",
            "-----------------------the loss of 623 iter is 5.726-----------------------\n",
            "-----------------------the loss of 624 iter is 5.852-----------------------\n",
            "-----------------------the loss of 625 iter is 5.892-----------------------\n",
            "-----------------------the loss of 626 iter is 5.501-----------------------\n",
            "-----------------------the loss of 627 iter is 5.714-----------------------\n",
            "-----------------------the loss of 628 iter is 5.791-----------------------\n",
            "-----------------------the loss of 629 iter is 5.861-----------------------\n",
            "-----------------------the loss of 630 iter is 5.656-----------------------\n",
            "-----------------------the loss of 631 iter is 5.748-----------------------\n",
            "-----------------------the loss of 632 iter is 5.677-----------------------\n",
            "-----------------------the loss of 633 iter is 5.786-----------------------\n",
            "-----------------------the loss of 634 iter is 5.590-----------------------\n",
            "-----------------------the loss of 635 iter is 5.833-----------------------\n",
            "-----------------------the loss of 636 iter is 5.715-----------------------\n",
            "-----------------------the loss of 637 iter is 5.888-----------------------\n",
            "-----------------------the loss of 638 iter is 5.754-----------------------\n",
            "-----------------------the loss of 639 iter is 5.698-----------------------\n",
            "-----------------------the loss of 640 iter is 5.366-----------------------\n",
            "-----------------------the loss of 641 iter is 5.891-----------------------\n",
            "-----------------------the loss of 642 iter is 6.041-----------------------\n",
            "-----------------------the loss of 643 iter is 5.820-----------------------\n",
            "-----------------------the loss of 644 iter is 6.051-----------------------\n",
            "-----------------------the loss of 645 iter is 5.694-----------------------\n",
            "-----------------------the loss of 646 iter is 5.717-----------------------\n",
            "-----------------------the loss of 647 iter is 5.647-----------------------\n",
            "-----------------------the loss of 648 iter is 5.613-----------------------\n",
            "-----------------------the loss of 649 iter is 5.719-----------------------\n",
            "-----------------------the loss of 650 iter is 5.782-----------------------\n",
            "-----------------------the loss of 651 iter is 5.567-----------------------\n",
            "-----------------------the loss of 652 iter is 5.333-----------------------\n",
            "-----------------------the loss of 653 iter is 5.736-----------------------\n",
            "-----------------------the loss of 654 iter is 6.202-----------------------\n",
            "-----------------------the loss of 655 iter is 5.741-----------------------\n",
            "-----------------------the loss of 656 iter is 5.741-----------------------\n",
            "-----------------------the loss of 657 iter is 5.927-----------------------\n",
            "-----------------------the loss of 658 iter is 5.680-----------------------\n",
            "-----------------------the loss of 659 iter is 5.836-----------------------\n",
            "-----------------------the loss of 660 iter is 6.028-----------------------\n",
            "-----------------------the loss of 661 iter is 5.663-----------------------\n",
            "-----------------------the loss of 662 iter is 5.763-----------------------\n",
            "-----------------------the loss of 663 iter is 5.948-----------------------\n",
            "-----------------------the loss of 664 iter is 5.692-----------------------\n",
            "-----------------------the loss of 665 iter is 5.760-----------------------\n",
            "-----------------------the loss of 666 iter is 6.197-----------------------\n",
            "-----------------------the loss of 667 iter is 5.702-----------------------\n",
            "-----------------------the loss of 668 iter is 5.543-----------------------\n",
            "-----------------------the loss of 669 iter is 5.819-----------------------\n",
            "-----------------------the loss of 670 iter is 5.901-----------------------\n",
            "-----------------------the loss of 671 iter is 5.685-----------------------\n",
            "-----------------------the loss of 672 iter is 5.399-----------------------\n",
            "-----------------------the loss of 673 iter is 6.056-----------------------\n",
            "-----------------------the loss of 674 iter is 5.995-----------------------\n",
            "-----------------------the loss of 675 iter is 6.000-----------------------\n",
            "-----------------------the loss of 676 iter is 5.947-----------------------\n",
            "-----------------------the loss of 677 iter is 5.563-----------------------\n",
            "-----------------------the loss of 678 iter is 5.995-----------------------\n",
            "-----------------------the loss of 679 iter is 5.420-----------------------\n",
            "-----------------------the loss of 680 iter is 5.381-----------------------\n",
            "-----------------------the loss of 681 iter is 5.582-----------------------\n",
            "-----------------------the loss of 682 iter is 5.763-----------------------\n",
            "-----------------------the loss of 683 iter is 5.705-----------------------\n",
            "-----------------------the loss of 684 iter is 5.931-----------------------\n",
            "-----------------------the loss of 685 iter is 5.373-----------------------\n",
            "-----------------------the loss of 686 iter is 5.660-----------------------\n",
            "-----------------------the loss of 687 iter is 5.585-----------------------\n",
            "-----------------------the loss of 688 iter is 6.092-----------------------\n",
            "-----------------------the loss of 689 iter is 5.748-----------------------\n",
            "-----------------------the loss of 690 iter is 5.810-----------------------\n",
            "-----------------------the loss of 691 iter is 5.895-----------------------\n",
            "-----------------------the loss of 692 iter is 5.805-----------------------\n",
            "-----------------------the loss of 693 iter is 5.475-----------------------\n",
            "-----------------------the loss of 694 iter is 5.694-----------------------\n",
            "-----------------------the loss of 695 iter is 5.941-----------------------\n",
            "-----------------------the loss of 696 iter is 5.733-----------------------\n",
            "-----------------------the loss of 697 iter is 5.645-----------------------\n",
            "-----------------------the loss of 698 iter is 5.774-----------------------\n",
            "-----------------------the loss of 699 iter is 5.534-----------------------\n",
            "-----------------------the loss of 700 iter is 5.894-----------------------\n",
            "-----------------------the loss of 701 iter is 5.478-----------------------\n",
            "-----------------------the loss of 702 iter is 5.122-----------------------\n",
            "-----------------------the loss of 703 iter is 5.423-----------------------\n",
            "-----------------------the loss of 704 iter is 5.886-----------------------\n",
            "-----------------------the loss of 705 iter is 5.966-----------------------\n",
            "-----------------------the loss of 706 iter is 5.573-----------------------\n",
            "-----------------------the loss of 707 iter is 5.974-----------------------\n",
            "-----------------------the loss of 708 iter is 5.865-----------------------\n",
            "-----------------------the loss of 709 iter is 5.487-----------------------\n",
            "-----------------------the loss of 710 iter is 5.658-----------------------\n",
            "-----------------------the loss of 711 iter is 5.170-----------------------\n",
            "-----------------------the loss of 712 iter is 5.846-----------------------\n",
            "-----------------------the loss of 713 iter is 5.898-----------------------\n",
            "-----------------------the loss of 714 iter is 5.889-----------------------\n",
            "-----------------------the loss of 715 iter is 5.682-----------------------\n",
            "-----------------------the loss of 716 iter is 5.462-----------------------\n",
            "-----------------------the loss of 717 iter is 5.640-----------------------\n",
            "-----------------------the loss of 718 iter is 5.993-----------------------\n",
            "-----------------------the loss of 719 iter is 5.663-----------------------\n",
            "-----------------------the loss of 720 iter is 6.095-----------------------\n",
            "-----------------------the loss of 721 iter is 6.061-----------------------\n",
            "-----------------------the loss of 722 iter is 5.563-----------------------\n",
            "-----------------------the loss of 723 iter is 5.814-----------------------\n",
            "-----------------------the loss of 724 iter is 6.043-----------------------\n",
            "-----------------------the loss of 725 iter is 5.682-----------------------\n",
            "-----------------------the loss of 726 iter is 5.544-----------------------\n",
            "-----------------------the loss of 727 iter is 6.097-----------------------\n",
            "-----------------------the loss of 728 iter is 5.520-----------------------\n",
            "-----------------------the loss of 729 iter is 5.924-----------------------\n",
            "-----------------------the loss of 730 iter is 5.790-----------------------\n",
            "-----------------------the loss of 731 iter is 6.109-----------------------\n",
            "-----------------------the loss of 732 iter is 5.593-----------------------\n",
            "-----------------------the loss of 733 iter is 5.550-----------------------\n",
            "-----------------------the loss of 734 iter is 6.187-----------------------\n",
            "-----------------------the loss of 735 iter is 5.546-----------------------\n",
            "-----------------------the loss of 736 iter is 5.643-----------------------\n",
            "-----------------------the loss of 737 iter is 6.248-----------------------\n",
            "-----------------------the loss of 738 iter is 5.592-----------------------\n",
            "-----------------------the loss of 739 iter is 5.655-----------------------\n",
            "-----------------------the loss of 740 iter is 5.984-----------------------\n",
            "-----------------------the loss of 741 iter is 5.825-----------------------\n",
            "-----------------------the loss of 742 iter is 6.050-----------------------\n",
            "-----------------------the loss of 743 iter is 6.013-----------------------\n",
            "-----------------------the loss of 744 iter is 5.851-----------------------\n",
            "-----------------------the loss of 745 iter is 5.422-----------------------\n",
            "-----------------------the loss of 746 iter is 5.659-----------------------\n",
            "-----------------------the loss of 747 iter is 5.653-----------------------\n",
            "-----------------------the loss of 748 iter is 5.757-----------------------\n",
            "-----------------------the loss of 749 iter is 5.762-----------------------\n",
            "-----------------------the loss of 750 iter is 5.785-----------------------\n",
            "-----------------------the loss of 751 iter is 5.949-----------------------\n",
            "-----------------------the loss of 752 iter is 4.931-----------------------\n",
            "-----------------------the loss of 753 iter is 5.041-----------------------\n",
            "-----------------------the loss of 754 iter is 5.694-----------------------\n",
            "-----------------------the loss of 755 iter is 5.594-----------------------\n",
            "-----------------------the loss of 756 iter is 5.665-----------------------\n",
            "-----------------------the loss of 757 iter is 5.474-----------------------\n",
            "-----------------------the loss of 758 iter is 5.720-----------------------\n",
            "-----------------------the loss of 759 iter is 5.802-----------------------\n",
            "-----------------------the loss of 760 iter is 5.839-----------------------\n",
            "-----------------------the loss of 761 iter is 6.195-----------------------\n",
            "-----------------------the loss of 762 iter is 5.774-----------------------\n",
            "-----------------------the loss of 763 iter is 5.674-----------------------\n",
            "-----------------------the loss of 764 iter is 5.535-----------------------\n",
            "-----------------------the loss of 765 iter is 5.894-----------------------\n",
            "-----------------------the loss of 766 iter is 5.874-----------------------\n",
            "-----------------------the loss of 767 iter is 5.974-----------------------\n",
            "-----------------------the loss of 768 iter is 5.211-----------------------\n",
            "-----------------------the loss of 769 iter is 5.932-----------------------\n",
            "-----------------------the loss of 770 iter is 5.645-----------------------\n",
            "-----------------------the loss of 771 iter is 5.910-----------------------\n",
            "-----------------------the loss of 772 iter is 5.959-----------------------\n",
            "-----------------------the loss of 773 iter is 5.665-----------------------\n",
            "-----------------------the loss of 774 iter is 5.315-----------------------\n",
            "-----------------------the loss of 775 iter is 5.733-----------------------\n",
            "-----------------------the loss of 776 iter is 5.970-----------------------\n",
            "-----------------------the loss of 777 iter is 5.642-----------------------\n",
            "-----------------------the loss of 778 iter is 5.697-----------------------\n",
            "-----------------------the loss of 779 iter is 5.734-----------------------\n",
            "-----------------------the loss of 780 iter is 6.114-----------------------\n",
            "-----------------------the loss of 781 iter is 6.014-----------------------\n",
            "-----------------------the loss of 782 iter is 5.869-----------------------\n",
            "-----------------------the loss of 783 iter is 5.882-----------------------\n",
            "-----------------------the loss of 784 iter is 5.655-----------------------\n",
            "-----------------------the loss of 785 iter is 5.632-----------------------\n",
            "-----------------------the loss of 786 iter is 5.685-----------------------\n",
            "-----------------------the loss of 787 iter is 5.851-----------------------\n",
            "-----------------------the loss of 788 iter is 5.508-----------------------\n",
            "-----------------------the loss of 789 iter is 5.298-----------------------\n",
            "-----------------------the loss of 790 iter is 5.407-----------------------\n",
            "-----------------------the loss of 791 iter is 5.397-----------------------\n",
            "-----------------------the loss of 792 iter is 5.655-----------------------\n",
            "-----------------------the loss of 793 iter is 5.776-----------------------\n",
            "-----------------------the loss of 794 iter is 5.971-----------------------\n",
            "-----------------------the loss of 795 iter is 5.762-----------------------\n",
            "-----------------------the loss of 796 iter is 5.698-----------------------\n",
            "-----------------------the loss of 797 iter is 5.580-----------------------\n",
            "-----------------------the loss of 798 iter is 5.971-----------------------\n",
            "-----------------------the loss of 799 iter is 5.645-----------------------\n",
            "-----------------------the loss of 800 iter is 5.866-----------------------\n",
            "-----------------------the loss of 801 iter is 5.818-----------------------\n",
            "-----------------------the loss of 802 iter is 5.388-----------------------\n",
            "-----------------------the loss of 803 iter is 6.002-----------------------\n",
            "-----------------------the loss of 804 iter is 5.766-----------------------\n",
            "-----------------------the loss of 805 iter is 5.594-----------------------\n",
            "-----------------------the loss of 806 iter is 6.172-----------------------\n",
            "-----------------------the loss of 807 iter is 5.923-----------------------\n",
            "-----------------------the loss of 808 iter is 5.818-----------------------\n",
            "-----------------------the loss of 809 iter is 5.683-----------------------\n",
            "-----------------------the loss of 810 iter is 5.555-----------------------\n",
            "-----------------------the loss of 811 iter is 5.526-----------------------\n",
            "-----------------------the loss of 812 iter is 5.694-----------------------\n",
            "-----------------------the loss of 813 iter is 5.731-----------------------\n",
            "-----------------------the loss of 814 iter is 5.743-----------------------\n",
            "-----------------------the loss of 815 iter is 5.867-----------------------\n",
            "-----------------------the loss of 816 iter is 5.527-----------------------\n",
            "-----------------------the loss of 817 iter is 5.777-----------------------\n",
            "-----------------------the loss of 818 iter is 5.544-----------------------\n",
            "-----------------------the loss of 819 iter is 5.357-----------------------\n",
            "-----------------------the loss of 820 iter is 5.695-----------------------\n",
            "-----------------------the loss of 821 iter is 5.664-----------------------\n",
            "-----------------------the loss of 822 iter is 5.584-----------------------\n",
            "-----------------------the loss of 823 iter is 5.490-----------------------\n",
            "-----------------------the loss of 824 iter is 5.609-----------------------\n",
            "-----------------------the loss of 825 iter is 5.741-----------------------\n",
            "-----------------------the loss of 826 iter is 5.603-----------------------\n",
            "-----------------------the loss of 827 iter is 5.580-----------------------\n",
            "-----------------------the loss of 828 iter is 5.476-----------------------\n",
            "-----------------------the loss of 829 iter is 5.464-----------------------\n",
            "-----------------------the loss of 830 iter is 5.736-----------------------\n",
            "-----------------------the loss of 831 iter is 5.744-----------------------\n",
            "-----------------------the loss of 832 iter is 5.519-----------------------\n",
            "-----------------------the loss of 833 iter is 5.838-----------------------\n",
            "-----------------------the loss of 834 iter is 5.907-----------------------\n",
            "-----------------------the loss of 835 iter is 5.663-----------------------\n",
            "-----------------------the loss of 836 iter is 5.512-----------------------\n",
            "-----------------------the loss of 837 iter is 5.789-----------------------\n",
            "-----------------------the loss of 838 iter is 5.953-----------------------\n",
            "-----------------------the loss of 839 iter is 5.517-----------------------\n",
            "-----------------------the loss of 840 iter is 5.990-----------------------\n",
            "-----------------------the loss of 841 iter is 5.865-----------------------\n",
            "-----------------------the loss of 842 iter is 5.625-----------------------\n",
            "-----------------------the loss of 843 iter is 5.861-----------------------\n",
            "-----------------------the loss of 844 iter is 5.641-----------------------\n",
            "-----------------------the loss of 845 iter is 6.074-----------------------\n",
            "-----------------------the loss of 846 iter is 5.627-----------------------\n",
            "-----------------------the loss of 847 iter is 5.954-----------------------\n",
            "-----------------------the loss of 848 iter is 5.693-----------------------\n",
            "-----------------------the loss of 849 iter is 5.663-----------------------\n",
            "-----------------------the loss of 850 iter is 5.416-----------------------\n",
            "-----------------------the loss of 851 iter is 5.347-----------------------\n",
            "-----------------------the loss of 852 iter is 5.524-----------------------\n",
            "-----------------------the loss of 853 iter is 5.956-----------------------\n",
            "-----------------------the loss of 854 iter is 5.706-----------------------\n",
            "-----------------------the loss of 855 iter is 5.708-----------------------\n",
            "-----------------------the loss of 856 iter is 5.974-----------------------\n",
            "-----------------------the loss of 857 iter is 5.865-----------------------\n",
            "-----------------------the loss of 858 iter is 6.324-----------------------\n",
            "-----------------------the loss of 859 iter is 5.746-----------------------\n",
            "-----------------------the loss of 860 iter is 5.500-----------------------\n",
            "-----------------------the loss of 861 iter is 5.593-----------------------\n",
            "-----------------------the loss of 862 iter is 5.801-----------------------\n",
            "-----------------------the loss of 863 iter is 5.366-----------------------\n",
            "-----------------------the loss of 864 iter is 5.747-----------------------\n",
            "-----------------------the loss of 865 iter is 5.552-----------------------\n",
            "-----------------------the loss of 866 iter is 5.581-----------------------\n",
            "-----------------------the loss of 867 iter is 5.633-----------------------\n",
            "-----------------------the loss of 868 iter is 5.498-----------------------\n",
            "-----------------------the loss of 869 iter is 5.820-----------------------\n",
            "-----------------------the loss of 870 iter is 5.561-----------------------\n",
            "-----------------------the loss of 871 iter is 5.234-----------------------\n",
            "-----------------------the loss of 872 iter is 5.640-----------------------\n",
            "-----------------------the loss of 873 iter is 5.907-----------------------\n",
            "-----------------------the loss of 874 iter is 5.923-----------------------\n",
            "-----------------------the loss of 875 iter is 5.933-----------------------\n",
            "-----------------------the loss of 876 iter is 5.323-----------------------\n",
            "-----------------------the loss of 877 iter is 5.543-----------------------\n",
            "-----------------------the loss of 878 iter is 5.778-----------------------\n",
            "-----------------------the loss of 879 iter is 5.929-----------------------\n",
            "-----------------------the loss of 880 iter is 5.436-----------------------\n",
            "-----------------------the loss of 881 iter is 5.648-----------------------\n",
            "-----------------------the loss of 882 iter is 5.480-----------------------\n",
            "-----------------------the loss of 883 iter is 5.658-----------------------\n",
            "-----------------------the loss of 884 iter is 5.865-----------------------\n",
            "-----------------------the loss of 885 iter is 5.368-----------------------\n",
            "-----------------------the loss of 886 iter is 5.707-----------------------\n",
            "-----------------------the loss of 887 iter is 5.873-----------------------\n",
            "-----------------------the loss of 888 iter is 4.847-----------------------\n",
            "-----------------------the loss of 889 iter is 6.180-----------------------\n",
            "-----------------------the loss of 890 iter is 5.584-----------------------\n",
            "-----------------------the loss of 891 iter is 5.992-----------------------\n",
            "-----------------------the loss of 892 iter is 5.676-----------------------\n",
            "-----------------------the loss of 893 iter is 5.313-----------------------\n",
            "-----------------------the loss of 894 iter is 5.614-----------------------\n",
            "-----------------------the loss of 895 iter is 6.178-----------------------\n",
            "-----------------------the loss of 896 iter is 5.592-----------------------\n",
            "-----------------------the loss of 897 iter is 5.834-----------------------\n",
            "-----------------------the loss of 898 iter is 5.403-----------------------\n",
            "-----------------------the loss of 899 iter is 6.048-----------------------\n",
            "-----------------------the loss of 900 iter is 5.399-----------------------\n",
            "-----------------------the loss of 901 iter is 6.033-----------------------\n",
            "-----------------------the loss of 902 iter is 5.900-----------------------\n",
            "-----------------------the loss of 903 iter is 5.627-----------------------\n",
            "-----------------------the loss of 904 iter is 5.669-----------------------\n",
            "-----------------------the loss of 905 iter is 5.730-----------------------\n",
            "-----------------------the loss of 906 iter is 5.643-----------------------\n",
            "-----------------------the loss of 907 iter is 5.318-----------------------\n",
            "-----------------------the loss of 908 iter is 6.066-----------------------\n",
            "-----------------------the loss of 909 iter is 5.561-----------------------\n",
            "-----------------------the loss of 910 iter is 5.665-----------------------\n",
            "-----------------------the loss of 911 iter is 6.058-----------------------\n",
            "-----------------------the loss of 912 iter is 5.377-----------------------\n",
            "-----------------------the loss of 913 iter is 5.623-----------------------\n",
            "-----------------------the loss of 914 iter is 5.537-----------------------\n",
            "-----------------------the loss of 915 iter is 5.955-----------------------\n",
            "-----------------------the loss of 916 iter is 5.520-----------------------\n",
            "-----------------------the loss of 917 iter is 5.037-----------------------\n",
            "-----------------------the loss of 918 iter is 5.698-----------------------\n",
            "-----------------------the loss of 919 iter is 5.590-----------------------\n",
            "-----------------------the loss of 920 iter is 5.697-----------------------\n",
            "-----------------------the loss of 921 iter is 5.747-----------------------\n",
            "-----------------------the loss of 922 iter is 5.969-----------------------\n",
            "-----------------------the loss of 923 iter is 5.926-----------------------\n",
            "-----------------------the loss of 924 iter is 5.885-----------------------\n",
            "-----------------------the loss of 925 iter is 5.590-----------------------\n",
            "-----------------------the loss of 926 iter is 5.297-----------------------\n",
            "-----------------------the loss of 927 iter is 5.988-----------------------\n",
            "-----------------------the loss of 928 iter is 6.009-----------------------\n",
            "-----------------------the loss of 929 iter is 6.029-----------------------\n",
            "-----------------------the loss of 930 iter is 5.624-----------------------\n",
            "-----------------------the loss of 931 iter is 5.460-----------------------\n",
            "-----------------------the loss of 932 iter is 5.653-----------------------\n",
            "-----------------------the loss of 933 iter is 5.857-----------------------\n",
            "-----------------------the loss of 934 iter is 5.699-----------------------\n",
            "-----------------------the loss of 935 iter is 5.420-----------------------\n",
            "-----------------------the loss of 936 iter is 5.888-----------------------\n",
            "-----------------------the loss of 937 iter is 5.552-----------------------\n",
            "-----------------------the loss of 938 iter is 5.950-----------------------\n",
            "-----------------------the loss of 939 iter is 5.722-----------------------\n",
            "-----------------------the loss of 940 iter is 5.365-----------------------\n",
            "-----------------------the loss of 941 iter is 5.781-----------------------\n",
            "-----------------------the loss of 942 iter is 5.825-----------------------\n",
            "-----------------------the loss of 943 iter is 5.788-----------------------\n",
            "-----------------------the loss of 944 iter is 5.631-----------------------\n",
            "-----------------------the loss of 945 iter is 5.565-----------------------\n",
            "-----------------------the loss of 946 iter is 5.722-----------------------\n",
            "-----------------------the loss of 947 iter is 5.662-----------------------\n",
            "-----------------------the loss of 948 iter is 5.629-----------------------\n",
            "-----------------------the loss of 949 iter is 5.202-----------------------\n",
            "-----------------------the loss of 950 iter is 5.548-----------------------\n",
            "-----------------------the loss of 951 iter is 5.241-----------------------\n",
            "-----------------------the loss of 952 iter is 5.372-----------------------\n",
            "-----------------------the loss of 953 iter is 5.489-----------------------\n",
            "-----------------------the loss of 954 iter is 5.417-----------------------\n",
            "-----------------------the loss of 955 iter is 6.059-----------------------\n",
            "-----------------------the loss of 956 iter is 5.713-----------------------\n",
            "-----------------------the loss of 957 iter is 5.683-----------------------\n",
            "-----------------------the loss of 958 iter is 5.543-----------------------\n",
            "-----------------------the loss of 959 iter is 5.715-----------------------\n",
            "-----------------------the loss of 960 iter is 5.632-----------------------\n",
            "-----------------------the loss of 961 iter is 5.815-----------------------\n",
            "-----------------------the loss of 962 iter is 5.823-----------------------\n",
            "-----------------------the loss of 963 iter is 5.886-----------------------\n",
            "-----------------------the loss of 964 iter is 5.398-----------------------\n",
            "-----------------------the loss of 965 iter is 5.496-----------------------\n",
            "-----------------------the loss of 966 iter is 5.971-----------------------\n",
            "-----------------------the loss of 967 iter is 5.628-----------------------\n",
            "-----------------------the loss of 968 iter is 5.460-----------------------\n",
            "-----------------------the loss of 969 iter is 5.783-----------------------\n",
            "-----------------------the loss of 970 iter is 5.443-----------------------\n",
            "-----------------------the loss of 971 iter is 5.665-----------------------\n",
            "-----------------------the loss of 972 iter is 5.447-----------------------\n",
            "-----------------------the loss of 973 iter is 5.775-----------------------\n",
            "-----------------------the loss of 974 iter is 5.562-----------------------\n",
            "-----------------------the loss of 975 iter is 5.720-----------------------\n",
            "-----------------------the loss of 976 iter is 5.043-----------------------\n",
            "-----------------------the loss of 977 iter is 5.713-----------------------\n",
            "-----------------------the loss of 978 iter is 5.358-----------------------\n",
            "-----------------------the loss of 979 iter is 5.248-----------------------\n",
            "-----------------------the loss of 980 iter is 5.665-----------------------\n",
            "-----------------------the loss of 981 iter is 5.478-----------------------\n",
            "-----------------------the loss of 982 iter is 5.704-----------------------\n",
            "-----------------------the loss of 983 iter is 5.515-----------------------\n",
            "-----------------------the loss of 984 iter is 5.794-----------------------\n",
            "-----------------------the loss of 985 iter is 5.466-----------------------\n",
            "-----------------------the loss of 986 iter is 5.934-----------------------\n",
            "-----------------------the loss of 987 iter is 5.578-----------------------\n",
            "-----------------------the loss of 988 iter is 5.768-----------------------\n",
            "-----------------------the loss of 989 iter is 5.563-----------------------\n",
            "-----------------------the loss of 990 iter is 5.835-----------------------\n",
            "-----------------------the loss of 991 iter is 5.841-----------------------\n",
            "-----------------------the loss of 992 iter is 5.371-----------------------\n",
            "-----------------------the loss of 993 iter is 5.320-----------------------\n",
            "-----------------------the loss of 994 iter is 5.823-----------------------\n",
            "-----------------------the loss of 995 iter is 5.817-----------------------\n",
            "-----------------------the loss of 996 iter is 5.486-----------------------\n",
            "-----------------------the loss of 997 iter is 5.755-----------------------\n",
            "-----------------------the loss of 998 iter is 5.457-----------------------\n",
            "-----------------------the loss of 999 iter is 5.745-----------------------\n",
            "-----------------------the loss of 1000 iter is 5.553-----------------------\n",
            "-----------------------the loss of 1001 iter is 5.258-----------------------\n",
            "-----------------------the loss of 1002 iter is 5.704-----------------------\n",
            "-----------------------the loss of 1003 iter is 6.105-----------------------\n",
            "-----------------------the loss of 1004 iter is 5.192-----------------------\n",
            "-----------------------the loss of 1005 iter is 5.427-----------------------\n",
            "-----------------------the loss of 1006 iter is 6.081-----------------------\n",
            "-----------------------the loss of 1007 iter is 5.857-----------------------\n",
            "-----------------------the loss of 1008 iter is 5.711-----------------------\n",
            "-----------------------the loss of 1009 iter is 5.341-----------------------\n",
            "-----------------------the loss of 1010 iter is 5.432-----------------------\n",
            "-----------------------the loss of 1011 iter is 5.404-----------------------\n",
            "-----------------------the loss of 1012 iter is 5.109-----------------------\n",
            "-----------------------the loss of 1013 iter is 5.623-----------------------\n",
            "-----------------------the loss of 1014 iter is 5.338-----------------------\n",
            "-----------------------the loss of 1015 iter is 5.411-----------------------\n",
            "-----------------------the loss of 1016 iter is 5.502-----------------------\n",
            "-----------------------the loss of 1017 iter is 5.811-----------------------\n",
            "-----------------------the loss of 1018 iter is 5.648-----------------------\n",
            "-----------------------the loss of 1019 iter is 5.601-----------------------\n",
            "-----------------------the loss of 1020 iter is 5.644-----------------------\n",
            "-----------------------the loss of 1021 iter is 5.776-----------------------\n",
            "-----------------------the loss of 1022 iter is 5.607-----------------------\n",
            "-----------------------the loss of 1023 iter is 5.951-----------------------\n",
            "-----------------------the loss of 1024 iter is 5.668-----------------------\n",
            "-----------------------the loss of 1025 iter is 5.731-----------------------\n",
            "-----------------------the loss of 1026 iter is 5.804-----------------------\n",
            "-----------------------the loss of 1027 iter is 5.653-----------------------\n",
            "-----------------------the loss of 1028 iter is 5.904-----------------------\n",
            "-----------------------the loss of 1029 iter is 5.886-----------------------\n",
            "-----------------------the loss of 1030 iter is 5.542-----------------------\n",
            "-----------------------the loss of 1031 iter is 5.505-----------------------\n",
            "-----------------------the loss of 1032 iter is 5.060-----------------------\n",
            "-----------------------the loss of 1033 iter is 5.518-----------------------\n",
            "-----------------------the loss of 1034 iter is 5.609-----------------------\n",
            "-----------------------the loss of 1035 iter is 5.873-----------------------\n",
            "-----------------------the loss of 1036 iter is 5.285-----------------------\n",
            "-----------------------the loss of 1037 iter is 5.781-----------------------\n",
            "-----------------------the loss of 1038 iter is 5.735-----------------------\n",
            "-----------------------the loss of 1039 iter is 6.034-----------------------\n",
            "-----------------------the loss of 1040 iter is 5.126-----------------------\n",
            "-----------------------the loss of 1041 iter is 5.632-----------------------\n",
            "-----------------------the loss of 1042 iter is 6.027-----------------------\n",
            "-----------------------the loss of 1043 iter is 5.116-----------------------\n",
            "-----------------------the loss of 1044 iter is 5.447-----------------------\n",
            "-----------------------the loss of 1045 iter is 5.582-----------------------\n",
            "-----------------------the loss of 1046 iter is 4.856-----------------------\n",
            "-----------------------the loss of 1047 iter is 5.130-----------------------\n",
            "-----------------------the loss of 1048 iter is 5.412-----------------------\n",
            "-----------------------the loss of 1049 iter is 5.490-----------------------\n",
            "-----------------------the loss of 1050 iter is 5.434-----------------------\n",
            "-----------------------the loss of 1051 iter is 5.806-----------------------\n",
            "-----------------------the loss of 1052 iter is 5.814-----------------------\n",
            "-----------------------the loss of 1053 iter is 5.577-----------------------\n",
            "-----------------------the loss of 1054 iter is 5.497-----------------------\n",
            "-----------------------the loss of 1055 iter is 5.769-----------------------\n",
            "-----------------------the loss of 1056 iter is 5.463-----------------------\n",
            "-----------------------the loss of 1057 iter is 5.747-----------------------\n",
            "-----------------------the loss of 1058 iter is 5.606-----------------------\n",
            "-----------------------the loss of 1059 iter is 5.572-----------------------\n",
            "-----------------------the loss of 1060 iter is 5.715-----------------------\n",
            "-----------------------the loss of 1061 iter is 5.574-----------------------\n",
            "-----------------------the loss of 1062 iter is 5.750-----------------------\n",
            "-----------------------the loss of 1063 iter is 5.647-----------------------\n",
            "-----------------------the loss of 1064 iter is 5.677-----------------------\n",
            "-----------------------the loss of 1065 iter is 5.558-----------------------\n",
            "-----------------------the loss of 1066 iter is 6.132-----------------------\n",
            "-----------------------the loss of 1067 iter is 5.535-----------------------\n",
            "-----------------------the loss of 1068 iter is 5.564-----------------------\n",
            "-----------------------the loss of 1069 iter is 5.214-----------------------\n",
            "-----------------------the loss of 1070 iter is 5.704-----------------------\n",
            "-----------------------the loss of 1071 iter is 5.657-----------------------\n",
            "-----------------------the loss of 1072 iter is 5.686-----------------------\n",
            "-----------------------the loss of 1073 iter is 5.699-----------------------\n",
            "-----------------------the loss of 1074 iter is 5.356-----------------------\n",
            "-----------------------the loss of 1075 iter is 5.917-----------------------\n",
            "-----------------------the loss of 1076 iter is 5.661-----------------------\n",
            "-----------------------the loss of 1077 iter is 5.724-----------------------\n",
            "-----------------------the loss of 1078 iter is 5.598-----------------------\n",
            "-----------------------the loss of 1079 iter is 5.461-----------------------\n",
            "-----------------------the loss of 1080 iter is 5.879-----------------------\n",
            "-----------------------the loss of 1081 iter is 5.310-----------------------\n",
            "-----------------------the loss of 1082 iter is 5.736-----------------------\n",
            "-----------------------the loss of 1083 iter is 5.526-----------------------\n",
            "-----------------------the loss of 1084 iter is 5.820-----------------------\n",
            "-----------------------the loss of 1085 iter is 5.592-----------------------\n",
            "-----------------------the loss of 1086 iter is 5.645-----------------------\n",
            "-----------------------the loss of 1087 iter is 5.248-----------------------\n",
            "-----------------------the loss of 1088 iter is 5.537-----------------------\n",
            "-----------------------the loss of 1089 iter is 5.635-----------------------\n",
            "-----------------------the loss of 1090 iter is 5.482-----------------------\n",
            "-----------------------the loss of 1091 iter is 5.617-----------------------\n",
            "-----------------------the loss of 1092 iter is 5.885-----------------------\n",
            "-----------------------the loss of 1093 iter is 5.917-----------------------\n",
            "-----------------------the loss of 1094 iter is 5.939-----------------------\n",
            "-----------------------the loss of 1095 iter is 5.581-----------------------\n",
            "-----------------------the loss of 1096 iter is 5.352-----------------------\n",
            "-----------------------the loss of 1097 iter is 5.673-----------------------\n",
            "-----------------------the loss of 1098 iter is 5.801-----------------------\n",
            "-----------------------the loss of 1099 iter is 5.420-----------------------\n",
            "-----------------------the loss of 1100 iter is 5.362-----------------------\n",
            "-----------------------the loss of 1101 iter is 5.548-----------------------\n",
            "-----------------------the loss of 1102 iter is 5.733-----------------------\n",
            "-----------------------the loss of 1103 iter is 5.740-----------------------\n",
            "-----------------------the loss of 1104 iter is 5.571-----------------------\n",
            "-----------------------the loss of 1105 iter is 5.854-----------------------\n",
            "-----------------------the loss of 1106 iter is 5.434-----------------------\n",
            "-----------------------the loss of 1107 iter is 5.377-----------------------\n",
            "-----------------------the loss of 1108 iter is 5.425-----------------------\n",
            "-----------------------the loss of 1109 iter is 5.809-----------------------\n",
            "-----------------------the loss of 1110 iter is 5.910-----------------------\n",
            "-----------------------the loss of 1111 iter is 5.412-----------------------\n",
            "-----------------------the loss of 1112 iter is 5.082-----------------------\n",
            "-----------------------the loss of 1113 iter is 5.267-----------------------\n",
            "-----------------------the loss of 1114 iter is 5.992-----------------------\n",
            "-----------------------the loss of 1115 iter is 5.626-----------------------\n",
            "-----------------------the loss of 1116 iter is 5.537-----------------------\n",
            "-----------------------the loss of 1117 iter is 5.415-----------------------\n",
            "-----------------------the loss of 1118 iter is 5.419-----------------------\n",
            "-----------------------the loss of 1119 iter is 5.282-----------------------\n",
            "-----------------------the loss of 1120 iter is 5.397-----------------------\n",
            "-----------------------the loss of 1121 iter is 5.652-----------------------\n",
            "-----------------------the loss of 1122 iter is 5.263-----------------------\n",
            "-----------------------the loss of 1123 iter is 5.986-----------------------\n",
            "-----------------------the loss of 1124 iter is 5.718-----------------------\n",
            "-----------------------the loss of 1125 iter is 5.556-----------------------\n",
            "-----------------------the loss of 1126 iter is 5.294-----------------------\n",
            "-----------------------the loss of 1127 iter is 5.678-----------------------\n",
            "-----------------------the loss of 1128 iter is 5.651-----------------------\n",
            "-----------------------the loss of 1129 iter is 5.319-----------------------\n",
            "-----------------------the loss of 1130 iter is 5.466-----------------------\n",
            "-----------------------the loss of 1131 iter is 5.433-----------------------\n",
            "-----------------------the loss of 1132 iter is 5.463-----------------------\n",
            "-----------------------the loss of 1133 iter is 5.091-----------------------\n",
            "-----------------------the loss of 1134 iter is 5.686-----------------------\n",
            "-----------------------the loss of 1135 iter is 5.447-----------------------\n",
            "-----------------------the loss of 1136 iter is 5.827-----------------------\n",
            "-----------------------the loss of 1137 iter is 5.582-----------------------\n",
            "-----------------------the loss of 1138 iter is 5.478-----------------------\n",
            "-----------------------the loss of 1139 iter is 5.711-----------------------\n",
            "-----------------------the loss of 1140 iter is 5.305-----------------------\n",
            "-----------------------the loss of 1141 iter is 5.627-----------------------\n",
            "-----------------------the loss of 1142 iter is 5.583-----------------------\n",
            "-----------------------the loss of 1143 iter is 5.497-----------------------\n",
            "-----------------------the loss of 1144 iter is 5.723-----------------------\n",
            "-----------------------the loss of 1145 iter is 5.243-----------------------\n",
            "-----------------------the loss of 1146 iter is 5.463-----------------------\n",
            "-----------------------the loss of 1147 iter is 5.460-----------------------\n",
            "-----------------------the loss of 1148 iter is 6.101-----------------------\n",
            "-----------------------the loss of 1149 iter is 5.274-----------------------\n",
            "-----------------------the loss of 1150 iter is 5.476-----------------------\n",
            "-----------------------the loss of 1151 iter is 5.851-----------------------\n",
            "-----------------------the loss of 1152 iter is 5.278-----------------------\n",
            "-----------------------the loss of 1153 iter is 5.489-----------------------\n",
            "-----------------------the loss of 1154 iter is 5.550-----------------------\n",
            "-----------------------the loss of 1155 iter is 5.196-----------------------\n",
            "-----------------------the loss of 1156 iter is 5.218-----------------------\n",
            "-----------------------the loss of 1157 iter is 5.357-----------------------\n",
            "-----------------------the loss of 1158 iter is 6.260-----------------------\n",
            "-----------------------the loss of 1159 iter is 5.505-----------------------\n",
            "-----------------------the loss of 1160 iter is 5.339-----------------------\n",
            "-----------------------the loss of 1161 iter is 5.499-----------------------\n",
            "-----------------------the loss of 1162 iter is 5.245-----------------------\n",
            "-----------------------the loss of 1163 iter is 5.793-----------------------\n",
            "-----------------------the loss of 1164 iter is 4.969-----------------------\n",
            "-----------------------the loss of 1165 iter is 5.235-----------------------\n",
            "-----------------------the loss of 1166 iter is 5.818-----------------------\n",
            "-----------------------the loss of 1167 iter is 5.591-----------------------\n",
            "-----------------------the loss of 1168 iter is 5.801-----------------------\n",
            "-----------------------the loss of 1169 iter is 5.174-----------------------\n",
            "-----------------------the loss of 1170 iter is 5.451-----------------------\n",
            "-----------------------the loss of 1171 iter is 5.357-----------------------\n",
            "-----------------------the loss of 1172 iter is 5.791-----------------------\n",
            "-----------------------the loss of 1173 iter is 5.651-----------------------\n",
            "-----------------------the loss of 1174 iter is 6.160-----------------------\n",
            "-----------------------the loss of 1175 iter is 6.200-----------------------\n",
            "-----------------------the loss of 1176 iter is 5.136-----------------------\n",
            "-----------------------the loss of 1177 iter is 5.906-----------------------\n",
            "-----------------------the loss of 1178 iter is 5.389-----------------------\n",
            "-----------------------the loss of 1179 iter is 5.561-----------------------\n",
            "-----------------------the loss of 1180 iter is 5.751-----------------------\n",
            "-----------------------the loss of 1181 iter is 5.432-----------------------\n",
            "-----------------------the loss of 1182 iter is 5.072-----------------------\n",
            "-----------------------the loss of 1183 iter is 5.260-----------------------\n",
            "-----------------------the loss of 1184 iter is 5.297-----------------------\n",
            "-----------------------the loss of 1185 iter is 5.476-----------------------\n",
            "-----------------------the loss of 1186 iter is 5.866-----------------------\n",
            "-----------------------the loss of 1187 iter is 5.657-----------------------\n",
            "-----------------------the loss of 1188 iter is 5.395-----------------------\n",
            "-----------------------the loss of 1189 iter is 6.198-----------------------\n",
            "-----------------------the loss of 1190 iter is 5.727-----------------------\n",
            "-----------------------the loss of 1191 iter is 5.511-----------------------\n",
            "-----------------------the loss of 1192 iter is 5.248-----------------------\n",
            "-----------------------the loss of 1193 iter is 5.191-----------------------\n",
            "-----------------------the loss of 1194 iter is 5.522-----------------------\n",
            "-----------------------the loss of 1195 iter is 5.559-----------------------\n",
            "-----------------------the loss of 1196 iter is 5.606-----------------------\n",
            "-----------------------the loss of 1197 iter is 6.080-----------------------\n",
            "-----------------------the loss of 1198 iter is 5.549-----------------------\n",
            "-----------------------the loss of 1199 iter is 6.171-----------------------\n",
            "-----------------------the loss of 1200 iter is 5.658-----------------------\n",
            "-----------------------the loss of 1201 iter is 5.748-----------------------\n",
            "-----------------------the loss of 1202 iter is 5.901-----------------------\n",
            "-----------------------the loss of 1203 iter is 5.130-----------------------\n",
            "-----------------------the loss of 1204 iter is 5.581-----------------------\n",
            "-----------------------the loss of 1205 iter is 5.609-----------------------\n",
            "-----------------------the loss of 1206 iter is 5.293-----------------------\n",
            "-----------------------the loss of 1207 iter is 5.605-----------------------\n",
            "-----------------------the loss of 1208 iter is 5.275-----------------------\n",
            "-----------------------the loss of 1209 iter is 5.521-----------------------\n",
            "-----------------------the loss of 1210 iter is 5.420-----------------------\n",
            "-----------------------the loss of 1211 iter is 5.543-----------------------\n",
            "-----------------------the loss of 1212 iter is 5.941-----------------------\n",
            "-----------------------the loss of 1213 iter is 5.234-----------------------\n",
            "-----------------------the loss of 1214 iter is 6.058-----------------------\n",
            "-----------------------the loss of 1215 iter is 5.214-----------------------\n",
            "-----------------------the loss of 1216 iter is 5.908-----------------------\n",
            "-----------------------the loss of 1217 iter is 6.020-----------------------\n",
            "-----------------------the loss of 1218 iter is 5.682-----------------------\n",
            "-----------------------the loss of 1219 iter is 5.402-----------------------\n",
            "-----------------------the loss of 1220 iter is 5.156-----------------------\n",
            "-----------------------the loss of 1221 iter is 5.251-----------------------\n",
            "-----------------------the loss of 1222 iter is 5.189-----------------------\n",
            "-----------------------the loss of 1223 iter is 5.523-----------------------\n",
            "-----------------------the loss of 1224 iter is 5.594-----------------------\n",
            "-----------------------the loss of 1225 iter is 5.434-----------------------\n",
            "-----------------------the loss of 1226 iter is 5.837-----------------------\n",
            "-----------------------the loss of 1227 iter is 5.434-----------------------\n",
            "-----------------------the loss of 1228 iter is 5.586-----------------------\n",
            "-----------------------the loss of 1229 iter is 5.588-----------------------\n",
            "-----------------------the loss of 1230 iter is 5.627-----------------------\n",
            "-----------------------the loss of 1231 iter is 5.379-----------------------\n",
            "-----------------------the loss of 1232 iter is 5.989-----------------------\n",
            "-----------------------the loss of 1233 iter is 5.399-----------------------\n",
            "-----------------------the loss of 1234 iter is 5.613-----------------------\n",
            "-----------------------the loss of 1235 iter is 5.817-----------------------\n",
            "-----------------------the loss of 1236 iter is 5.410-----------------------\n",
            "-----------------------the loss of 1237 iter is 5.350-----------------------\n",
            "-----------------------the loss of 1238 iter is 6.200-----------------------\n",
            "-----------------------the loss of 1239 iter is 5.224-----------------------\n",
            "-----------------------the loss of 1240 iter is 5.578-----------------------\n",
            "-----------------------the loss of 1241 iter is 5.374-----------------------\n",
            "-----------------------the loss of 1242 iter is 5.696-----------------------\n",
            "-----------------------the loss of 1243 iter is 5.555-----------------------\n",
            "-----------------------the loss of 1244 iter is 6.000-----------------------\n",
            "-----------------------the loss of 1245 iter is 5.582-----------------------\n",
            "-----------------------the loss of 1246 iter is 5.010-----------------------\n",
            "-----------------------the loss of 1247 iter is 5.458-----------------------\n",
            "-----------------------the loss of 1248 iter is 5.501-----------------------\n",
            "-----------------------the loss of 1249 iter is 5.874-----------------------\n",
            "-----------------------the loss of 1250 iter is 5.240-----------------------\n",
            "-----------------------the loss of 1251 iter is 5.757-----------------------\n",
            "-----------------------the loss of 1252 iter is 5.472-----------------------\n",
            "-----------------------the loss of 1253 iter is 4.981-----------------------\n",
            "-----------------------the loss of 1254 iter is 5.377-----------------------\n",
            "-----------------------the loss of 1255 iter is 5.444-----------------------\n",
            "-----------------------the loss of 1256 iter is 5.252-----------------------\n",
            "-----------------------the loss of 1257 iter is 5.085-----------------------\n",
            "-----------------------the loss of 1258 iter is 5.761-----------------------\n",
            "-----------------------the loss of 1259 iter is 5.144-----------------------\n",
            "-----------------------the loss of 1260 iter is 5.690-----------------------\n",
            "-----------------------the loss of 1261 iter is 5.447-----------------------\n",
            "-----------------------the loss of 1262 iter is 5.357-----------------------\n",
            "-----------------------the loss of 1263 iter is 5.650-----------------------\n",
            "-----------------------the loss of 1264 iter is 5.281-----------------------\n",
            "-----------------------the loss of 1265 iter is 5.681-----------------------\n",
            "-----------------------the loss of 1266 iter is 5.604-----------------------\n",
            "-----------------------the loss of 1267 iter is 5.586-----------------------\n",
            "-----------------------the loss of 1268 iter is 5.417-----------------------\n",
            "-----------------------the loss of 1269 iter is 5.187-----------------------\n",
            "-----------------------the loss of 1270 iter is 5.380-----------------------\n",
            "-----------------------the loss of 1271 iter is 5.767-----------------------\n",
            "-----------------------the loss of 1272 iter is 5.366-----------------------\n",
            "-----------------------the loss of 1273 iter is 5.217-----------------------\n",
            "-----------------------the loss of 1274 iter is 5.448-----------------------\n",
            "-----------------------the loss of 1275 iter is 5.253-----------------------\n",
            "-----------------------the loss of 1276 iter is 5.497-----------------------\n",
            "-----------------------the loss of 1277 iter is 5.546-----------------------\n",
            "-----------------------the loss of 1278 iter is 5.625-----------------------\n",
            "-----------------------the loss of 1279 iter is 5.448-----------------------\n",
            "-----------------------the loss of 1280 iter is 5.612-----------------------\n",
            "-----------------------the loss of 1281 iter is 5.560-----------------------\n",
            "-----------------------the loss of 1282 iter is 5.597-----------------------\n",
            "-----------------------the loss of 1283 iter is 6.272-----------------------\n",
            "-----------------------the loss of 1284 iter is 5.331-----------------------\n",
            "-----------------------the loss of 1285 iter is 5.158-----------------------\n",
            "-----------------------the loss of 1286 iter is 5.668-----------------------\n",
            "-----------------------the loss of 1287 iter is 5.465-----------------------\n",
            "-----------------------the loss of 1288 iter is 5.644-----------------------\n",
            "-----------------------the loss of 1289 iter is 5.619-----------------------\n",
            "-----------------------the loss of 1290 iter is 5.434-----------------------\n",
            "-----------------------the loss of 1291 iter is 5.640-----------------------\n",
            "-----------------------the loss of 1292 iter is 5.537-----------------------\n",
            "-----------------------the loss of 1293 iter is 5.495-----------------------\n",
            "-----------------------the loss of 1294 iter is 5.675-----------------------\n",
            "-----------------------the loss of 1295 iter is 5.681-----------------------\n",
            "-----------------------the loss of 1296 iter is 5.509-----------------------\n",
            "-----------------------the loss of 1297 iter is 5.669-----------------------\n",
            "-----------------------the loss of 1298 iter is 5.373-----------------------\n",
            "-----------------------the loss of 1299 iter is 5.512-----------------------\n",
            "-----------------------the loss of 1300 iter is 5.819-----------------------\n",
            "-----------------------the loss of 1301 iter is 5.572-----------------------\n",
            "-----------------------the loss of 1302 iter is 5.430-----------------------\n",
            "-----------------------the loss of 1303 iter is 5.404-----------------------\n",
            "-----------------------the loss of 1304 iter is 5.640-----------------------\n",
            "-----------------------the loss of 1305 iter is 5.293-----------------------\n",
            "-----------------------the loss of 1306 iter is 5.456-----------------------\n",
            "-----------------------the loss of 1307 iter is 4.613-----------------------\n",
            "-----------------------the loss of 1308 iter is 5.426-----------------------\n",
            "-----------------------the loss of 1309 iter is 5.751-----------------------\n",
            "-----------------------the loss of 1310 iter is 5.722-----------------------\n",
            "-----------------------the loss of 1311 iter is 5.695-----------------------\n",
            "-----------------------the loss of 1312 iter is 5.589-----------------------\n",
            "-----------------------the loss of 1313 iter is 5.677-----------------------\n",
            "-----------------------the loss of 1314 iter is 5.631-----------------------\n",
            "-----------------------the loss of 1315 iter is 5.540-----------------------\n",
            "-----------------------the loss of 1316 iter is 5.435-----------------------\n",
            "-----------------------the loss of 1317 iter is 5.559-----------------------\n",
            "-----------------------the loss of 1318 iter is 5.616-----------------------\n",
            "-----------------------the loss of 1319 iter is 5.484-----------------------\n",
            "-----------------------the loss of 1320 iter is 5.572-----------------------\n",
            "-----------------------the loss of 1321 iter is 5.898-----------------------\n",
            "-----------------------the loss of 1322 iter is 5.112-----------------------\n",
            "-----------------------the loss of 1323 iter is 5.427-----------------------\n",
            "-----------------------the loss of 1324 iter is 5.429-----------------------\n",
            "-----------------------the loss of 1325 iter is 5.562-----------------------\n",
            "-----------------------the loss of 1326 iter is 4.952-----------------------\n",
            "-----------------------the loss of 1327 iter is 5.691-----------------------\n",
            "-----------------------the loss of 1328 iter is 5.472-----------------------\n",
            "-----------------------the loss of 1329 iter is 5.543-----------------------\n",
            "-----------------------the loss of 1330 iter is 5.385-----------------------\n",
            "-----------------------the loss of 1331 iter is 5.851-----------------------\n",
            "-----------------------the loss of 1332 iter is 5.131-----------------------\n",
            "-----------------------the loss of 1333 iter is 5.865-----------------------\n",
            "-----------------------the loss of 1334 iter is 5.334-----------------------\n",
            "-----------------------the loss of 1335 iter is 5.747-----------------------\n",
            "-----------------------the loss of 1336 iter is 5.203-----------------------\n",
            "-----------------------the loss of 1337 iter is 5.621-----------------------\n",
            "-----------------------the loss of 1338 iter is 5.389-----------------------\n",
            "-----------------------the loss of 1339 iter is 5.042-----------------------\n",
            "-----------------------the loss of 1340 iter is 5.695-----------------------\n",
            "-----------------------the loss of 1341 iter is 5.354-----------------------\n",
            "-----------------------the loss of 1342 iter is 5.817-----------------------\n",
            "-----------------------the loss of 1343 iter is 5.677-----------------------\n",
            "-----------------------the loss of 1344 iter is 5.619-----------------------\n",
            "-----------------------the loss of 1345 iter is 5.592-----------------------\n",
            "-----------------------the loss of 1346 iter is 5.623-----------------------\n",
            "-----------------------the loss of 1347 iter is 5.258-----------------------\n",
            "-----------------------the loss of 1348 iter is 5.512-----------------------\n",
            "-----------------------the loss of 1349 iter is 5.817-----------------------\n",
            "-----------------------the loss of 1350 iter is 5.495-----------------------\n",
            "-----------------------the loss of 1351 iter is 5.253-----------------------\n",
            "-----------------------the loss of 1352 iter is 5.440-----------------------\n",
            "-----------------------the loss of 1353 iter is 5.759-----------------------\n",
            "-----------------------the loss of 1354 iter is 5.815-----------------------\n",
            "-----------------------the loss of 1355 iter is 5.403-----------------------\n",
            "-----------------------the loss of 1356 iter is 5.145-----------------------\n",
            "-----------------------the loss of 1357 iter is 5.368-----------------------\n",
            "-----------------------the loss of 1358 iter is 5.441-----------------------\n",
            "-----------------------the loss of 1359 iter is 5.637-----------------------\n",
            "-----------------------the loss of 1360 iter is 5.752-----------------------\n",
            "-----------------------the loss of 1361 iter is 5.468-----------------------\n",
            "-----------------------the loss of 1362 iter is 5.025-----------------------\n",
            "-----------------------the loss of 1363 iter is 5.275-----------------------\n",
            "-----------------------the loss of 1364 iter is 5.712-----------------------\n",
            "-----------------------the loss of 1365 iter is 5.425-----------------------\n",
            "-----------------------the loss of 1366 iter is 5.569-----------------------\n",
            "-----------------------the loss of 1367 iter is 4.952-----------------------\n",
            "-----------------------the loss of 1368 iter is 5.283-----------------------\n",
            "-----------------------the loss of 1369 iter is 5.503-----------------------\n",
            "-----------------------the loss of 1370 iter is 5.544-----------------------\n",
            "-----------------------the loss of 1371 iter is 5.198-----------------------\n",
            "-----------------------the loss of 1372 iter is 5.336-----------------------\n",
            "-----------------------the loss of 1373 iter is 5.105-----------------------\n",
            "-----------------------the loss of 1374 iter is 5.346-----------------------\n",
            "-----------------------the loss of 1375 iter is 5.343-----------------------\n",
            "-----------------------the loss of 1376 iter is 5.344-----------------------\n",
            "-----------------------the loss of 1377 iter is 5.818-----------------------\n",
            "-----------------------the loss of 1378 iter is 4.911-----------------------\n",
            "-----------------------the loss of 1379 iter is 5.483-----------------------\n",
            "-----------------------the loss of 1380 iter is 5.502-----------------------\n",
            "-----------------------the loss of 1381 iter is 4.909-----------------------\n",
            "-----------------------the loss of 1382 iter is 5.775-----------------------\n",
            "-----------------------the loss of 1383 iter is 5.027-----------------------\n",
            "-----------------------the loss of 1384 iter is 5.464-----------------------\n",
            "-----------------------the loss of 1385 iter is 5.028-----------------------\n",
            "-----------------------the loss of 1386 iter is 5.679-----------------------\n",
            "-----------------------the loss of 1387 iter is 5.445-----------------------\n",
            "-----------------------the loss of 1388 iter is 4.636-----------------------\n",
            "-----------------------the loss of 1389 iter is 5.993-----------------------\n",
            "-----------------------the loss of 1390 iter is 5.181-----------------------\n",
            "-----------------------the loss of 1391 iter is 5.742-----------------------\n",
            "-----------------------the loss of 1392 iter is 5.452-----------------------\n",
            "-----------------------the loss of 1393 iter is 5.590-----------------------\n",
            "-----------------------the loss of 1394 iter is 5.517-----------------------\n",
            "-----------------------the loss of 1395 iter is 5.243-----------------------\n",
            "-----------------------the loss of 1396 iter is 5.650-----------------------\n",
            "-----------------------the loss of 1397 iter is 4.866-----------------------\n",
            "-----------------------the loss of 1398 iter is 5.359-----------------------\n",
            "-----------------------the loss of 1399 iter is 5.734-----------------------\n",
            "-----------------------the loss of 1400 iter is 5.426-----------------------\n",
            "-----------------------the loss of 1401 iter is 5.856-----------------------\n",
            "-----------------------the loss of 1402 iter is 5.338-----------------------\n",
            "-----------------------the loss of 1403 iter is 5.152-----------------------\n",
            "-----------------------the loss of 1404 iter is 5.388-----------------------\n",
            "-----------------------the loss of 1405 iter is 5.442-----------------------\n",
            "-----------------------the loss of 1406 iter is 5.480-----------------------\n",
            "-----------------------the loss of 1407 iter is 5.337-----------------------\n",
            "-----------------------the loss of 1408 iter is 5.507-----------------------\n",
            "-----------------------the loss of 1409 iter is 5.667-----------------------\n",
            "-----------------------the loss of 1410 iter is 5.077-----------------------\n",
            "-----------------------the loss of 1411 iter is 5.724-----------------------\n",
            "-----------------------the loss of 1412 iter is 5.282-----------------------\n",
            "-----------------------the loss of 1413 iter is 5.500-----------------------\n",
            "-----------------------the loss of 1414 iter is 5.738-----------------------\n",
            "-----------------------the loss of 1415 iter is 4.797-----------------------\n",
            "-----------------------the loss of 1416 iter is 5.335-----------------------\n",
            "-----------------------the loss of 1417 iter is 5.452-----------------------\n",
            "-----------------------the loss of 1418 iter is 5.640-----------------------\n",
            "-----------------------the loss of 1419 iter is 5.164-----------------------\n",
            "-----------------------the loss of 1420 iter is 5.457-----------------------\n",
            "-----------------------the loss of 1421 iter is 5.268-----------------------\n",
            "-----------------------the loss of 1422 iter is 5.915-----------------------\n",
            "-----------------------the loss of 1423 iter is 5.804-----------------------\n",
            "-----------------------the loss of 1424 iter is 5.815-----------------------\n",
            "-----------------------the loss of 1425 iter is 4.719-----------------------\n",
            "-----------------------the loss of 1426 iter is 5.118-----------------------\n",
            "-----------------------the loss of 1427 iter is 5.290-----------------------\n",
            "-----------------------the loss of 1428 iter is 5.356-----------------------\n",
            "-----------------------the loss of 1429 iter is 5.768-----------------------\n",
            "-----------------------the loss of 1430 iter is 5.095-----------------------\n",
            "-----------------------the loss of 1431 iter is 5.457-----------------------\n",
            "-----------------------the loss of 1432 iter is 4.921-----------------------\n",
            "-----------------------the loss of 1433 iter is 5.832-----------------------\n",
            "-----------------------the loss of 1434 iter is 5.276-----------------------\n",
            "-----------------------the loss of 1435 iter is 5.791-----------------------\n",
            "-----------------------the loss of 1436 iter is 5.403-----------------------\n",
            "-----------------------the loss of 1437 iter is 5.149-----------------------\n",
            "-----------------------the loss of 1438 iter is 5.521-----------------------\n",
            "-----------------------the loss of 1439 iter is 5.645-----------------------\n",
            "-----------------------the loss of 1440 iter is 5.310-----------------------\n",
            "-----------------------the loss of 1441 iter is 5.398-----------------------\n",
            "-----------------------the loss of 1442 iter is 5.460-----------------------\n",
            "-----------------------the loss of 1443 iter is 5.596-----------------------\n",
            "-----------------------the loss of 1444 iter is 5.667-----------------------\n",
            "-----------------------the loss of 1445 iter is 5.865-----------------------\n",
            "-----------------------the loss of 1446 iter is 5.312-----------------------\n",
            "-----------------------the loss of 1447 iter is 5.566-----------------------\n",
            "-----------------------the loss of 1448 iter is 5.589-----------------------\n",
            "-----------------------the loss of 1449 iter is 5.676-----------------------\n",
            "-----------------------the loss of 1450 iter is 5.447-----------------------\n",
            "-----------------------the loss of 1451 iter is 5.242-----------------------\n",
            "-----------------------the loss of 1452 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1453 iter is 5.504-----------------------\n",
            "-----------------------the loss of 1454 iter is 5.282-----------------------\n",
            "-----------------------the loss of 1455 iter is 6.055-----------------------\n",
            "-----------------------the loss of 1456 iter is 5.794-----------------------\n",
            "-----------------------the loss of 1457 iter is 4.807-----------------------\n",
            "-----------------------the loss of 1458 iter is 4.736-----------------------\n",
            "-----------------------the loss of 1459 iter is 5.202-----------------------\n",
            "-----------------------the loss of 1460 iter is 5.367-----------------------\n",
            "-----------------------the loss of 1461 iter is 5.706-----------------------\n",
            "-----------------------the loss of 1462 iter is 5.341-----------------------\n",
            "-----------------------the loss of 1463 iter is 5.376-----------------------\n",
            "-----------------------the loss of 1464 iter is 5.420-----------------------\n",
            "-----------------------the loss of 1465 iter is 5.641-----------------------\n",
            "-----------------------the loss of 1466 iter is 5.360-----------------------\n",
            "-----------------------the loss of 1467 iter is 5.121-----------------------\n",
            "-----------------------the loss of 1468 iter is 5.127-----------------------\n",
            "-----------------------the loss of 1469 iter is 5.815-----------------------\n",
            "-----------------------the loss of 1470 iter is 5.363-----------------------\n",
            "-----------------------the loss of 1471 iter is 5.745-----------------------\n",
            "-----------------------the loss of 1472 iter is 5.296-----------------------\n",
            "-----------------------the loss of 1473 iter is 5.838-----------------------\n",
            "-----------------------the loss of 1474 iter is 5.562-----------------------\n",
            "-----------------------the loss of 1475 iter is 5.350-----------------------\n",
            "-----------------------the loss of 1476 iter is 5.234-----------------------\n",
            "-----------------------the loss of 1477 iter is 5.272-----------------------\n",
            "-----------------------the loss of 1478 iter is 5.331-----------------------\n",
            "-----------------------the loss of 1479 iter is 5.471-----------------------\n",
            "-----------------------the loss of 1480 iter is 5.445-----------------------\n",
            "-----------------------the loss of 1481 iter is 5.254-----------------------\n",
            "-----------------------the loss of 1482 iter is 5.516-----------------------\n",
            "-----------------------the loss of 1483 iter is 5.532-----------------------\n",
            "-----------------------the loss of 1484 iter is 4.926-----------------------\n",
            "-----------------------the loss of 1485 iter is 5.558-----------------------\n",
            "-----------------------the loss of 1486 iter is 5.321-----------------------\n",
            "-----------------------the loss of 1487 iter is 5.787-----------------------\n",
            "-----------------------the loss of 1488 iter is 5.688-----------------------\n",
            "-----------------------the loss of 1489 iter is 5.479-----------------------\n",
            "-----------------------the loss of 1490 iter is 5.249-----------------------\n",
            "-----------------------the loss of 1491 iter is 5.315-----------------------\n",
            "-----------------------the loss of 1492 iter is 5.230-----------------------\n",
            "-----------------------the loss of 1493 iter is 5.338-----------------------\n",
            "-----------------------the loss of 1494 iter is 5.281-----------------------\n",
            "-----------------------the loss of 1495 iter is 5.184-----------------------\n",
            "-----------------------the loss of 1496 iter is 5.558-----------------------\n",
            "-----------------------the loss of 1497 iter is 4.936-----------------------\n",
            "-----------------------the loss of 1498 iter is 5.686-----------------------\n",
            "-----------------------the loss of 1499 iter is 5.870-----------------------\n",
            "-----------------------the loss of 1500 iter is 5.109-----------------------\n",
            "-----------------------the loss of 1501 iter is 5.285-----------------------\n",
            "-----------------------the loss of 1502 iter is 5.313-----------------------\n",
            "-----------------------the loss of 1503 iter is 5.305-----------------------\n",
            "-----------------------the loss of 1504 iter is 5.417-----------------------\n",
            "-----------------------the loss of 1505 iter is 5.347-----------------------\n",
            "-----------------------the loss of 1506 iter is 5.015-----------------------\n",
            "-----------------------the loss of 1507 iter is 5.155-----------------------\n",
            "-----------------------the loss of 1508 iter is 5.416-----------------------\n",
            "-----------------------the loss of 1509 iter is 5.554-----------------------\n",
            "-----------------------the loss of 1510 iter is 5.480-----------------------\n",
            "-----------------------the loss of 1511 iter is 5.555-----------------------\n",
            "-----------------------the loss of 1512 iter is 5.309-----------------------\n",
            "-----------------------the loss of 1513 iter is 5.732-----------------------\n",
            "-----------------------the loss of 1514 iter is 5.598-----------------------\n",
            "-----------------------the loss of 1515 iter is 5.708-----------------------\n",
            "-----------------------the loss of 1516 iter is 5.257-----------------------\n",
            "-----------------------the loss of 1517 iter is 4.672-----------------------\n",
            "-----------------------the loss of 1518 iter is 5.435-----------------------\n",
            "-----------------------the loss of 1519 iter is 5.295-----------------------\n",
            "-----------------------the loss of 1520 iter is 5.385-----------------------\n",
            "-----------------------the loss of 1521 iter is 4.959-----------------------\n",
            "-----------------------the loss of 1522 iter is 5.628-----------------------\n",
            "-----------------------the loss of 1523 iter is 5.771-----------------------\n",
            "-----------------------the loss of 1524 iter is 5.345-----------------------\n",
            "-----------------------the loss of 1525 iter is 5.279-----------------------\n",
            "-----------------------the loss of 1526 iter is 5.247-----------------------\n",
            "-----------------------the loss of 1527 iter is 5.526-----------------------\n",
            "-----------------------the loss of 1528 iter is 5.273-----------------------\n",
            "-----------------------the loss of 1529 iter is 5.729-----------------------\n",
            "-----------------------the loss of 1530 iter is 5.564-----------------------\n",
            "-----------------------the loss of 1531 iter is 5.675-----------------------\n",
            "-----------------------the loss of 1532 iter is 5.538-----------------------\n",
            "-----------------------the loss of 1533 iter is 5.580-----------------------\n",
            "-----------------------the loss of 1534 iter is 5.520-----------------------\n",
            "-----------------------the loss of 1535 iter is 5.901-----------------------\n",
            "-----------------------the loss of 1536 iter is 5.414-----------------------\n",
            "-----------------------the loss of 1537 iter is 5.460-----------------------\n",
            "-----------------------the loss of 1538 iter is 5.297-----------------------\n",
            "-----------------------the loss of 1539 iter is 5.437-----------------------\n",
            "-----------------------the loss of 1540 iter is 5.745-----------------------\n",
            "-----------------------the loss of 1541 iter is 4.944-----------------------\n",
            "-----------------------the loss of 1542 iter is 5.510-----------------------\n",
            "-----------------------the loss of 1543 iter is 5.466-----------------------\n",
            "-----------------------the loss of 1544 iter is 5.280-----------------------\n",
            "-----------------------the loss of 1545 iter is 5.756-----------------------\n",
            "-----------------------the loss of 1546 iter is 5.456-----------------------\n",
            "-----------------------the loss of 1547 iter is 5.089-----------------------\n",
            "-----------------------the loss of 1548 iter is 5.541-----------------------\n",
            "-----------------------the loss of 1549 iter is 5.793-----------------------\n",
            "-----------------------the loss of 1550 iter is 5.908-----------------------\n",
            "-----------------------the loss of 1551 iter is 5.608-----------------------\n",
            "-----------------------the loss of 1552 iter is 5.476-----------------------\n",
            "-----------------------the loss of 1553 iter is 5.436-----------------------\n",
            "-----------------------the loss of 1554 iter is 5.415-----------------------\n",
            "-----------------------the loss of 1555 iter is 5.536-----------------------\n",
            "-----------------------the loss of 1556 iter is 5.540-----------------------\n",
            "-----------------------the loss of 1557 iter is 5.500-----------------------\n",
            "-----------------------the loss of 1558 iter is 5.982-----------------------\n",
            "-----------------------the loss of 1559 iter is 5.392-----------------------\n",
            "-----------------------the loss of 1560 iter is 5.846-----------------------\n",
            "-----------------------the loss of 1561 iter is 5.633-----------------------\n",
            "-----------------------the loss of 1562 iter is 4.750-----------------------\n",
            "-----------------------the loss of 1563 iter is 5.401-----------------------\n",
            "-----------------------the loss of 1564 iter is 6.178-----------------------\n",
            "-----------------------the loss of 1565 iter is 5.395-----------------------\n",
            "-----------------------the loss of 1566 iter is 5.191-----------------------\n",
            "-----------------------the loss of 1567 iter is 5.872-----------------------\n",
            "-----------------------the loss of 1568 iter is 5.361-----------------------\n",
            "-----------------------the loss of 1569 iter is 5.474-----------------------\n",
            "-----------------------the loss of 1570 iter is 5.130-----------------------\n",
            "-----------------------the loss of 1571 iter is 5.857-----------------------\n",
            "-----------------------the loss of 1572 iter is 5.358-----------------------\n",
            "-----------------------the loss of 1573 iter is 5.605-----------------------\n",
            "-----------------------the loss of 1574 iter is 5.584-----------------------\n",
            "-----------------------the loss of 1575 iter is 5.455-----------------------\n",
            "-----------------------the loss of 1576 iter is 5.315-----------------------\n",
            "-----------------------the loss of 1577 iter is 5.047-----------------------\n",
            "-----------------------the loss of 1578 iter is 5.258-----------------------\n",
            "-----------------------the loss of 1579 iter is 5.005-----------------------\n",
            "-----------------------the loss of 1580 iter is 5.780-----------------------\n",
            "-----------------------the loss of 1581 iter is 5.699-----------------------\n",
            "-----------------------the loss of 1582 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1583 iter is 5.262-----------------------\n",
            "-----------------------the loss of 1584 iter is 5.433-----------------------\n",
            "-----------------------the loss of 1585 iter is 5.444-----------------------\n",
            "-----------------------the loss of 1586 iter is 4.703-----------------------\n",
            "-----------------------the loss of 1587 iter is 5.408-----------------------\n",
            "-----------------------the loss of 1588 iter is 5.514-----------------------\n",
            "-----------------------the loss of 1589 iter is 5.111-----------------------\n",
            "-----------------------the loss of 1590 iter is 5.722-----------------------\n",
            "-----------------------the loss of 1591 iter is 5.176-----------------------\n",
            "-----------------------the loss of 1592 iter is 5.843-----------------------\n",
            "-----------------------the loss of 1593 iter is 5.262-----------------------\n",
            "-----------------------the loss of 1594 iter is 5.083-----------------------\n",
            "-----------------------the loss of 1595 iter is 5.654-----------------------\n",
            "-----------------------the loss of 1596 iter is 5.516-----------------------\n",
            "-----------------------the loss of 1597 iter is 5.546-----------------------\n",
            "-----------------------the loss of 1598 iter is 4.745-----------------------\n",
            "-----------------------the loss of 1599 iter is 5.404-----------------------\n",
            "-----------------------the loss of 1600 iter is 5.486-----------------------\n",
            "-----------------------the loss of 1601 iter is 5.704-----------------------\n",
            "-----------------------the loss of 1602 iter is 5.465-----------------------\n",
            "-----------------------the loss of 1603 iter is 5.422-----------------------\n",
            "-----------------------the loss of 1604 iter is 5.508-----------------------\n",
            "-----------------------the loss of 1605 iter is 5.396-----------------------\n",
            "-----------------------the loss of 1606 iter is 5.208-----------------------\n",
            "-----------------------the loss of 1607 iter is 5.449-----------------------\n",
            "-----------------------the loss of 1608 iter is 5.506-----------------------\n",
            "-----------------------the loss of 1609 iter is 5.560-----------------------\n",
            "-----------------------the loss of 1610 iter is 5.407-----------------------\n",
            "-----------------------the loss of 1611 iter is 6.034-----------------------\n",
            "-----------------------the loss of 1612 iter is 5.062-----------------------\n",
            "-----------------------the loss of 1613 iter is 5.215-----------------------\n",
            "-----------------------the loss of 1614 iter is 5.759-----------------------\n",
            "-----------------------the loss of 1615 iter is 5.272-----------------------\n",
            "-----------------------the loss of 1616 iter is 5.293-----------------------\n",
            "-----------------------the loss of 1617 iter is 5.269-----------------------\n",
            "-----------------------the loss of 1618 iter is 5.374-----------------------\n",
            "-----------------------the loss of 1619 iter is 5.535-----------------------\n",
            "-----------------------the loss of 1620 iter is 5.505-----------------------\n",
            "-----------------------the loss of 1621 iter is 5.477-----------------------\n",
            "-----------------------the loss of 1622 iter is 5.351-----------------------\n",
            "-----------------------the loss of 1623 iter is 5.403-----------------------\n",
            "-----------------------the loss of 1624 iter is 5.407-----------------------\n",
            "-----------------------the loss of 1625 iter is 5.565-----------------------\n",
            "-----------------------the loss of 1626 iter is 5.451-----------------------\n",
            "-----------------------the loss of 1627 iter is 5.013-----------------------\n",
            "-----------------------the loss of 1628 iter is 5.322-----------------------\n",
            "-----------------------the loss of 1629 iter is 5.211-----------------------\n",
            "-----------------------the loss of 1630 iter is 5.461-----------------------\n",
            "-----------------------the loss of 1631 iter is 5.817-----------------------\n",
            "-----------------------the loss of 1632 iter is 4.925-----------------------\n",
            "-----------------------the loss of 1633 iter is 5.521-----------------------\n",
            "-----------------------the loss of 1634 iter is 5.145-----------------------\n",
            "-----------------------the loss of 1635 iter is 5.182-----------------------\n",
            "-----------------------the loss of 1636 iter is 5.442-----------------------\n",
            "-----------------------the loss of 1637 iter is 5.801-----------------------\n",
            "-----------------------the loss of 1638 iter is 5.793-----------------------\n",
            "-----------------------the loss of 1639 iter is 5.042-----------------------\n",
            "-----------------------the loss of 1640 iter is 5.202-----------------------\n",
            "-----------------------the loss of 1641 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1642 iter is 5.510-----------------------\n",
            "-----------------------the loss of 1643 iter is 5.487-----------------------\n",
            "-----------------------the loss of 1644 iter is 5.282-----------------------\n",
            "-----------------------the loss of 1645 iter is 5.466-----------------------\n",
            "-----------------------the loss of 1646 iter is 5.367-----------------------\n",
            "-----------------------the loss of 1647 iter is 5.195-----------------------\n",
            "-----------------------the loss of 1648 iter is 5.296-----------------------\n",
            "-----------------------the loss of 1649 iter is 5.277-----------------------\n",
            "-----------------------the loss of 1650 iter is 4.934-----------------------\n",
            "-----------------------the loss of 1651 iter is 5.729-----------------------\n",
            "-----------------------the loss of 1652 iter is 5.471-----------------------\n",
            "-----------------------the loss of 1653 iter is 5.319-----------------------\n",
            "-----------------------the loss of 1654 iter is 5.872-----------------------\n",
            "-----------------------the loss of 1655 iter is 5.534-----------------------\n",
            "-----------------------the loss of 1656 iter is 5.629-----------------------\n",
            "-----------------------the loss of 1657 iter is 5.491-----------------------\n",
            "-----------------------the loss of 1658 iter is 5.315-----------------------\n",
            "-----------------------the loss of 1659 iter is 5.730-----------------------\n",
            "-----------------------the loss of 1660 iter is 5.456-----------------------\n",
            "-----------------------the loss of 1661 iter is 5.756-----------------------\n",
            "-----------------------the loss of 1662 iter is 5.171-----------------------\n",
            "-----------------------the loss of 1663 iter is 5.284-----------------------\n",
            "-----------------------the loss of 1664 iter is 5.282-----------------------\n",
            "-----------------------the loss of 1665 iter is 5.258-----------------------\n",
            "-----------------------the loss of 1666 iter is 5.237-----------------------\n",
            "-----------------------the loss of 1667 iter is 5.185-----------------------\n",
            "-----------------------the loss of 1668 iter is 5.405-----------------------\n",
            "-----------------------the loss of 1669 iter is 5.176-----------------------\n",
            "-----------------------the loss of 1670 iter is 5.332-----------------------\n",
            "-----------------------the loss of 1671 iter is 5.399-----------------------\n",
            "-----------------------the loss of 1672 iter is 5.284-----------------------\n",
            "-----------------------the loss of 1673 iter is 5.719-----------------------\n",
            "-----------------------the loss of 1674 iter is 5.494-----------------------\n",
            "-----------------------the loss of 1675 iter is 5.511-----------------------\n",
            "-----------------------the loss of 1676 iter is 5.755-----------------------\n",
            "-----------------------the loss of 1677 iter is 5.180-----------------------\n",
            "-----------------------the loss of 1678 iter is 5.632-----------------------\n",
            "-----------------------the loss of 1679 iter is 5.114-----------------------\n",
            "-----------------------the loss of 1680 iter is 5.590-----------------------\n",
            "-----------------------the loss of 1681 iter is 5.409-----------------------\n",
            "-----------------------the loss of 1682 iter is 5.640-----------------------\n",
            "-----------------------the loss of 1683 iter is 5.707-----------------------\n",
            "-----------------------the loss of 1684 iter is 5.333-----------------------\n",
            "-----------------------the loss of 1685 iter is 5.816-----------------------\n",
            "-----------------------the loss of 1686 iter is 5.535-----------------------\n",
            "-----------------------the loss of 1687 iter is 5.213-----------------------\n",
            "-----------------------the loss of 1688 iter is 4.916-----------------------\n",
            "-----------------------the loss of 1689 iter is 5.396-----------------------\n",
            "-----------------------the loss of 1690 iter is 5.716-----------------------\n",
            "-----------------------the loss of 1691 iter is 5.440-----------------------\n",
            "-----------------------the loss of 1692 iter is 5.472-----------------------\n",
            "-----------------------the loss of 1693 iter is 5.637-----------------------\n",
            "-----------------------the loss of 1694 iter is 5.534-----------------------\n",
            "-----------------------the loss of 1695 iter is 5.546-----------------------\n",
            "-----------------------the loss of 1696 iter is 5.477-----------------------\n",
            "-----------------------the loss of 1697 iter is 5.289-----------------------\n",
            "-----------------------the loss of 1698 iter is 5.660-----------------------\n",
            "-----------------------the loss of 1699 iter is 5.493-----------------------\n",
            "-----------------------the loss of 1700 iter is 5.200-----------------------\n",
            "-----------------------the loss of 1701 iter is 5.278-----------------------\n",
            "-----------------------the loss of 1702 iter is 5.188-----------------------\n",
            "-----------------------the loss of 1703 iter is 4.745-----------------------\n",
            "-----------------------the loss of 1704 iter is 5.444-----------------------\n",
            "-----------------------the loss of 1705 iter is 5.492-----------------------\n",
            "-----------------------the loss of 1706 iter is 5.249-----------------------\n",
            "-----------------------the loss of 1707 iter is 5.497-----------------------\n",
            "-----------------------the loss of 1708 iter is 5.366-----------------------\n",
            "-----------------------the loss of 1709 iter is 5.437-----------------------\n",
            "-----------------------the loss of 1710 iter is 5.553-----------------------\n",
            "-----------------------the loss of 1711 iter is 5.428-----------------------\n",
            "-----------------------the loss of 1712 iter is 5.228-----------------------\n",
            "-----------------------the loss of 1713 iter is 5.512-----------------------\n",
            "-----------------------the loss of 1714 iter is 5.199-----------------------\n",
            "-----------------------the loss of 1715 iter is 5.634-----------------------\n",
            "-----------------------the loss of 1716 iter is 5.706-----------------------\n",
            "-----------------------the loss of 1717 iter is 5.756-----------------------\n",
            "-----------------------the loss of 1718 iter is 5.504-----------------------\n",
            "-----------------------the loss of 1719 iter is 5.331-----------------------\n",
            "-----------------------the loss of 1720 iter is 5.398-----------------------\n",
            "-----------------------the loss of 1721 iter is 5.624-----------------------\n",
            "-----------------------the loss of 1722 iter is 5.189-----------------------\n",
            "-----------------------the loss of 1723 iter is 5.278-----------------------\n",
            "-----------------------the loss of 1724 iter is 5.601-----------------------\n",
            "-----------------------the loss of 1725 iter is 5.334-----------------------\n",
            "-----------------------the loss of 1726 iter is 5.680-----------------------\n",
            "-----------------------the loss of 1727 iter is 5.194-----------------------\n",
            "-----------------------the loss of 1728 iter is 5.247-----------------------\n",
            "-----------------------the loss of 1729 iter is 5.301-----------------------\n",
            "-----------------------the loss of 1730 iter is 5.349-----------------------\n",
            "-----------------------the loss of 1731 iter is 5.745-----------------------\n",
            "-----------------------the loss of 1732 iter is 5.775-----------------------\n",
            "-----------------------the loss of 1733 iter is 5.563-----------------------\n",
            "-----------------------the loss of 1734 iter is 5.127-----------------------\n",
            "-----------------------the loss of 1735 iter is 5.468-----------------------\n",
            "-----------------------the loss of 1736 iter is 5.395-----------------------\n",
            "-----------------------the loss of 1737 iter is 5.043-----------------------\n",
            "-----------------------the loss of 1738 iter is 5.372-----------------------\n",
            "-----------------------the loss of 1739 iter is 5.394-----------------------\n",
            "-----------------------the loss of 1740 iter is 5.787-----------------------\n",
            "-----------------------the loss of 1741 iter is 5.773-----------------------\n",
            "-----------------------the loss of 1742 iter is 5.150-----------------------\n",
            "-----------------------the loss of 1743 iter is 5.244-----------------------\n",
            "-----------------------the loss of 1744 iter is 5.651-----------------------\n",
            "-----------------------the loss of 1745 iter is 5.187-----------------------\n",
            "-----------------------the loss of 1746 iter is 5.625-----------------------\n",
            "-----------------------the loss of 1747 iter is 5.717-----------------------\n",
            "-----------------------the loss of 1748 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1749 iter is 5.595-----------------------\n",
            "-----------------------the loss of 1750 iter is 5.124-----------------------\n",
            "-----------------------the loss of 1751 iter is 5.631-----------------------\n",
            "-----------------------the loss of 1752 iter is 4.835-----------------------\n",
            "-----------------------the loss of 1753 iter is 5.136-----------------------\n",
            "-----------------------the loss of 1754 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1755 iter is 5.547-----------------------\n",
            "-----------------------the loss of 1756 iter is 5.370-----------------------\n",
            "-----------------------the loss of 1757 iter is 5.297-----------------------\n",
            "-----------------------the loss of 1758 iter is 5.047-----------------------\n",
            "-----------------------the loss of 1759 iter is 5.157-----------------------\n",
            "-----------------------the loss of 1760 iter is 5.234-----------------------\n",
            "-----------------------the loss of 1761 iter is 5.660-----------------------\n",
            "-----------------------the loss of 1762 iter is 5.660-----------------------\n",
            "-----------------------the loss of 1763 iter is 4.839-----------------------\n",
            "-----------------------the loss of 1764 iter is 5.369-----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqjU60nvQ1bZ"
      },
      "source": [
        "성능이 무척 안좋다. 이유는 다음과 같을 것으로 추측된다. \n",
        "1. 길이 제한을 두지 않았다. 이로 인해 길이가 50 이상인 긴 문장들이 다수 존재하고, 이는 seq2seq의 한계로 작용한다. \n",
        "2. 모델 구조가 단순하다. 인코더를 더 쌓고 싶었으나, 학습에 너무 오랜 시간이 걸려 2층 밖에 쌓지 못했다. \n",
        "3. 2번과 비슷한 이유로, 코랩을 사용하여 학습하다보니 램의 한계로 모델을 크게 만들 수 없었다. 이로 인해 임베딩 벡터와 모델 내부 벡터의 크기를 128과 64로 작게 잡을 수 밖에 없었다. 실제로 벡터 크기를 늘리자 조금 더 학습이 원활해지는 모습이다. \n",
        "4. pretrained-embedding을 사용하지 않았다. \n",
        "5. tokenizer의 문제가 있다. mecab과 spacy를 사용했는데 전처리를 거의 가하지 않아, 고유명사, 인명, 숫자 등이 그대로 입력되었다. 이로인해 모델이 맥락을 파악하는 것이 어려웠을 것으로 추측된다. \n"
      ]
    }
  ]
}